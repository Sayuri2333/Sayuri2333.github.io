<!DOCTYPE html>
<html lang="zh-cn">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#494f5c">
	<meta name="msapplication-TileColor" content="#494f5c">
<meta itemprop="name" content="Deep Q-learning算法">
<meta itemprop="description" content="价值函数的近似表示 之前介绍的强化学习方法使用的都是有限个状态集合\(S\)，而当遇到连续的状态时，则需要价值函数的近似表示。 价值函数的近似表">
<meta itemprop="datePublished" content="2020-05-08T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-05-08T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1586">



<meta itemprop="keywords" content="" /><meta property="og:title" content="Deep Q-learning算法" />
<meta property="og:description" content="价值函数的近似表示 之前介绍的强化学习方法使用的都是有限个状态集合\(S\)，而当遇到连续的状态时，则需要价值函数的近似表示。 价值函数的近似表" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/post/deep-q-learning/" />
<meta property="article:published_time" content="2020-05-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-08T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Q-learning算法"/>
<meta name="twitter:description" content="价值函数的近似表示 之前介绍的强化学习方法使用的都是有限个状态集合\(S\)，而当遇到连续的状态时，则需要价值函数的近似表示。 价值函数的近似表"/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>Deep Q-learning算法</title>
	<link rel="stylesheet" href="https://example.com/css/style.min.eac77496566fd7d5768fd650ddb0b2b181ca6a2d7c5fdd6fe6b8ba4bf47e566f.css" integrity="sha256-6sd0llZv19V2j9ZQ3bCysYHKai18X91v5ri6S/R+Vm8=" crossorigin="anonymous">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://example.com">SayuriBlog</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://example.com/post/">Posts</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-social hide-in-mobile"><a href="https://github.com/Sayuri2333" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://example.com/post/">Posts</a></li>
		</ul>
	</div>


	<main class="site-main section-inner thin animated fadeIn faster">
		<h1>Deep Q-learning算法</h1>
		<div class="content">
			<h3 id="价值函数的近似表示">价值函数的近似表示<a href="#价值函数的近似表示" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>之前介绍的强化学习方法使用的都是有限个状态集合<span  class="math">\(S\)</span>，而当遇到连续的状态时，则需要价值函数的近似表示。</p>

<h3 id="价值函数的近似表示方法">价值函数的近似表示方法<a href="#价值函数的近似表示方法" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>价值函数的近似表示拥有几种方法。一是引入一个状态价值函数<span  class="math">\(\hat{v}\)</span>，这个函数由参数<span  class="math">\(w\)</span>表示，并接受状态<span  class="math">\(s\)</span>作为输入，计算后得到状态<span  class="math">\(s\)</span>的价值。即:</p>

<p><span  class="math">\[
\hat{v}(s, w) \approx v_{\pi}(s)
\]</span></p>

<p>类似的，引入一个动作价值函数<span  class="math">\(\hat{q}\)</span>，这个函数由参数<span  class="math">\(w\)</span>表示，并接受状态<span  class="math">\(s\)</span>与动作<span  class="math">\(a\)</span>作为输入，计算后得到动作价值。即：</p>

<p><span  class="math">\[
\hat{q}(s,a,w) \approx q_{\pi}(s,a)
\]</span></p>

<p>价值函数的近似表示通常使用神经网络。神经网络的使用主要有以下三种情况。</p>

<p><figure><img src="E:HugoblogcontentimgDQL.jpg" alt=""></figure></p>

<p>对于状态价值函数， 神经网络的输入是状态<span  class="math">\(s\)</span>的特征向量，输出是状态价值<span  class="math">\(\hat{v}(s, w)\)</span>。对于动作价值函数，有两种方法，一种是输入状态<span  class="math">\(s\)</span>的特征向量和动作<span  class="math">\(a\)</span>，输出对应的动作价值<span  class="math">\(\hat{q}(s,a,w)\)</span>，另一种是只输入状态<span  class="math">\(s\)</span>的特征向量，动作集合有多少个动作就有多少个输出<span  class="math">\(\hat{q}(s,a_i,w)\)</span>。这里隐含了我们的动作是有限个的离散动作。</p>

<h3 id="deep-qlearning算法思路">Deep Q-Learning算法思路<a href="#deep-qlearning算法思路" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>Deep Q-Learning算法的基本思路来源于Q-learning。 但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值<span  class="math">\(s\)</span>和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p>

<p>DQN的输入是状态s对应的状态向量<span  class="math">\(\phi(s)\)</span>， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p>

<p>DQN主要使用的技巧是经验回放（experience replay）,即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。</p>

<p>通过经验回放得到的目标Q值和通过Q网络计算得到的Q值存在误差时，可以通过梯度反向传播来更新神经网络的参数<span  class="math">\(w\)</span>直到当<span  class="math">\(w\)</span>收敛。</p>

<p>算法输入：迭代轮数<span  class="math">\(T\)</span>，状态特征维度<span  class="math">\(n\)</span>，动作集<span  class="math">\(A\)</span>，步长<span  class="math">\(\alpha\)</span>，衰减因子<span  class="math">\(\gamma\)</span>，探索率<span  class="math">\(\epsilon\)</span>，Q网络结构，批量梯度下降的样本数<span  class="math">\(m\)</span>。</p>

<p>输出：Q网络参数</p>

<ol>
<li><p>随机初始化Q网络的所有参数<span  class="math">\(w\)</span>， 基于<span  class="math">\(w\)</span>初始化所有的状态和动作对应的价值<span  class="math">\(Q\)</span>。清空经验回放的集合<span  class="math">\(D\)</span>。</p></li>

<li><p>for i from 1 to T，进行迭代。</p>

<ol>
<li><p>初始化S为当前状态序列的第一个状态, 拿到其特征向量<span  class="math">\(\phi(S)\)</span></p></li>

<li><p>在Q网络中使用<span  class="math">\(\phi(S)\)</span>作为输入，得到Q网络的所有动作对应的Q值输出。用ϵ−贪婪法在当前Q值输出中选择对应的动作A</p></li>

<li><p>在状态<span  class="math">\(S\)</span>执行当前动作<span  class="math">\(A\)</span>,得到新状态<span  class="math">\(S'\)</span>对应的特征向量<span  class="math">\(\phi(S')\)</span>和奖励<span  class="math">\(R\)</span>,是否终止状态is_end</p></li>

<li><p>将<span  class="math">\(\{\phi(S),A,R,\phi(S'),is\_end\}\)</span>这个五元组存入经验回放集合<span  class="math">\(D\)</span></p></li>

<li><p><span  class="math">\[S=S'\]</span></p></li>

<li><p>从经验回放集合<span  class="math">\(D\)</span>中采样<span  class="math">\(m\)</span>个样本<span  class="math">\(\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2.,,,m\)</span>，计算当前Q值<span  class="math">\(y_j\)</span>：</p></li>
</ol>

<p><span  class="math">\[
  y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
  \]</span></p>

<p>7.使用均方差损失函数,通过神经网络的梯度反向传播来更新Q网络的所有参数<span  class="math">\(w\)</span></p>

<p><span  class="math">\[
  \frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2
  \]</span></p>

<p>8.如果<span  class="math">\(S'\)</span>是终止状态，当前轮迭代完毕，否则转到步骤2.2</p></li>
</ol>

<p>上述2.6以及2.7步骤中的Q值计算都要通过Q网络计算得到。实际应用中为了算法更好的收敛探索率需要随着迭代的进行而减小。</p>

<h3 id="deep-qlearning算法实现">Deep Q-Learning算法实现<a href="#deep-qlearning算法实现" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>下面使用OpenAi Gym中的CartPole-v0游戏来作为算法实现的游戏。具体算法请参照我的<a href="https://github.com/Sayuri2333/reinforcement_learning">github</a>。</p>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2020 <a href="https://example.com"></a> Sayuri2333 &#183; <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://example.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
	</footer>



	<script src="https://example.com/js/bundle.min.4a9a0ac3d2217822c7865b4161e6c2a71de1d70492264337755427898dd718f6.js" integrity="sha256-SpoKw9IheCLHhltBYebCpx3h1wSSJkM3dVQniY3XGPY=" crossorigin="anonymous"></script>
	

</body>

</html>
