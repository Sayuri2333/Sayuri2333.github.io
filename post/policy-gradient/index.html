<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Policy Gradient&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">Policy Gradient</h1><p class="article date">Tuesday, March 2, 2021</p></section><article class="article markdown-body"><h3 id="dqn类算法的不足">DQN类算法的不足</h3>

<ol>
<li>无法表示随机策略。</li>
<li>输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。</li>
<li>无法表示连续动作。</li>
</ol>

<h3 id="策略梯度算法">策略梯度算法</h3>

<p>相比于DQN类算法使用参数化函数表示价值函数，策略梯度算法则使用参数化函数表示策略。令<span  class="math">\(\theta \in \mathbb{R}^{d}\)</span>表示参数向量，则策略函数可表示为：</p>

<p><span  class="math">\[
\pi(a \mid s, {\theta})=\mathrm{P}_{\mathrm{r}}\left\{A_{t}=a \mid S_{t}=s, {\theta}_{t}={\theta}\right\}
\]</span></p>

<p>使用上述策略生成马尔可夫过程的一个轨迹<span  class="math">\(\tau=\left({s}_{1}, {a}_{1}, \ldots, {s}_{T}, {a}_{T}\right)\)</span>的概率为：</p>

<p><span  class="math">\[
\underbrace{p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)}_{\pi_{\theta}(\tau)}=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)
\]</span></p>

<p>在策略梯度算法中，目标是找到一组最佳的参数<span  class="math">\(\theta^*\)</span>来表示策略函数使得累计奖励的期望最大，即：</p>

<p><span  class="math">\[
\theta^{\star}=\arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\]</span></p>

<p>当然，相对于平均奖励的目标函数，也存在有起始状态的目标函数。这种情况相当于对各个时间步的奖励进行加权。在这里我们仅仅考虑平均奖励目标函数。</p>

<p>令累积奖励为<span  class="math">\(R(\tau)=\sum_{t=1}^{T} r\left(s_{t}, a_{t}\right)\)</span>，则目标函数为：</p>

<p><span  class="math">\[
J\left(\pi_{\theta}\right)=\underset{\tau \sim \pi_{\theta}}{\mathrm{E}}[R(\tau)]
\]</span></p>

<p>对<span  class="math">\(J\left(\pi_{\theta}\right)\)</span>求梯度可以得到：</p>

<p><span  class="math">\[
\begin{aligned}
\nabla_{\theta} J(\theta) &=\int \nabla_{\theta} \pi_{\theta}(\tau) r(\tau) d \tau \\
&=\int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau) d \tau \\
&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)\right]
\end{aligned}
\]</span></p>

<p>由于有<span  class="math">\(\pi_{\theta}(\tau) = p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)\)</span>，因此将<span  class="math">\(p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)\)</span>对数化后可代入上式。对数化后有：</p>

<p><span  class="math">\[
\log \pi_{\theta}(\tau)=\log p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)+\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)
\]</span></p>

<p>将上述对数化后的式子代入求导，因为<span  class="math">\(\log p\left(\mathbf{s}_{1}\right)\)</span>以及<span  class="math">\(\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)\)</span>都与<span  class="math">\(\theta\)</span>无关，因此求导后可以消去。最终得到：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]
\]</span></p>

<p>使用蒙特卡洛采样可以近似求解。根据策略<span  class="math">\(\pi_{\theta}\)</span>生成<span  class="math">\(N\)</span>条轨迹后，由蒙特卡洛采样有：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
\]</span></p>

<p>求得梯度后可以使用梯度上升法更新策略参数值。</p>

<h3 id="reinforce算法">REINFORCE算法</h3>

<p>根据上述式子可以使用蒙特卡洛法更新策略参数。</p>

<p>输入：<span  class="math">\(N\)</span>个蒙特卡罗完整序列,训练步长<span  class="math">\(\alpha\)</span></p>

<p>输出：策略函数的参数<span  class="math">\(\theta\)</span></p>

<ol>
<li><p>初始化<span  class="math">\(\theta\)</span></p></li>

<li><p>开始循环</p>

<ol>
<li><p>采样一条完整的序列，并计算每个时间位置<span  class="math">\(t\)</span>的状态价值<span  class="math">\(v_{t}\)</span></p></li>

<li><p>对于每个时间位置<span  class="math">\(t\)</span>，使用梯度上升法更新策略函数的参数<span  class="math">\(\theta\)</span>：</p></li>
</ol>

<p><span  class="math">\[
  \theta=\theta+\alpha \nabla_{\theta} \log \pi_{\theta}\left(s_{t}, a_{t}\right) v_{t}
  \]</span></p></li>

<li><p>返回策略函数的参数<span  class="math">\(\theta\)</span></p></li>
</ol>

<p>对于离散空间，最常用的策略函数为softmax函数。即：</p>

<p><span  class="math">\[
\pi_{\theta}(s, a)=\frac{e^{\phi(s, a)^{T_{\theta}}}}{\sum_{b} e^{\phi(s, b)^{T} \theta}}
\]</span></p>

<p>这种情况下，其梯度为：</p>

<p><span  class="math">\[
\nabla_{\theta} \log \pi_{\theta}(s, a)=\phi(s, a)-\mathbb{E}_{\pi \theta}[\phi(s, .)]
\]</span></p>

<p>对于连续空间，可以使用高斯分布函数作为策略函数。这种情况下通常使用神经网络预测分布的均值<span  class="math">\(\mu(s)\)</span>，然后结合给定的方差<span  class="math">\(\sigma^{2}\)</span>构造分布<span  class="math">\(a \sim \mathcal{N}\left(\mu(s), \sigma^{2}\right)\)</span>。</p>

<h3 id="更新式的几何意义">更新式的几何意义</h3>

<p>显而易见，<span  class="math">\(\log \pi_{\theta}\left(s_{t}, a_{t}\right)\)</span>是策略<span  class="math">\(\pi\)</span>的对数似然表示。因此，<span  class="math">\( \nabla_{\theta} \log \pi_{\theta}\left(s_{t}, a_{t}\right)\)</span>表示了状态<span  class="math">\(S_{t}\)</span>下执行动作<span  class="math">\(A_{t}\)</span>的对数似然的梯度方向。而<span  class="math">\(v_t\)</span>控制了更新的幅度。相比之下，价值越大的动作更新的幅度越大，即下次遇到同样情况时执行此动作的概率会变高。</p>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/noisynet/"><span class="li">&larr;</span>Noisy Network</a></p><p><a class="link" href="/post/drqn/"><span class="li">&rarr;</span>Deep Recurrent Q Network</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>