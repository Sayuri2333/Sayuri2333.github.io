<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>QR-DQN&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">QR-DQN</h1><p class="article date">Friday, March 5, 2021</p></section><article class="article markdown-body"><h3 id="c51算法的缺陷">C51算法的缺陷</h3>

<p>C51算法中虽然证明了<span  class="math">\(\mathcal{T}\)</span>Bellman算子在使用分布时是满足<span  class="math">\(\gamma -contraction\)</span>条件的，但是由于实际上无法在使用Wasserstein距离的时候用随机梯度下降进行优化，所以最后作者还是使用KL散度启发式的进行距离衡量。这与作者的原意相违背。QR-DQN算法就对这一过程进行了改进。</p>

<h3 id="wasserstein距离">Wasserstein距离</h3>

<p>Wasserstein距离简单来说就是用来衡量两个分布之间距离的指标。它能够衡量任意分布之间的距离，即使是离散分布与连续分布之间的距离也可以。相比于KL散度，Wasserstein距离不仅具有对称性，而且能够以更符合直观感受的方式衡量距离。</p>

<p><figure><img src="/image/Wasserstein_merit.png" alt=""></figure></p>

<p>如在上图的情况下，可以看出相比于右端的分布，左端的分布与中间的分布之间的距离实际上要更近。因此从直观上来说，左端与中间的分布之间的距离应该要小于左端分布与右端分布之间的距离。但是在KL散度中，两个不重合分布之间的距离为固定的正无穷，因此无法应付这种情况。但是Wasserstein距离则能够反映上述区别。</p>

<p>Wasserstein距离的直观解释可以看<a href="https://zhuanlan.zhihu.com/p/26988777">这篇文章</a>。Wasserstein距离的一般方程为：</p>

<p><span  class="math">\[
W _ { p } ( \mu , \nu ) : = \left( \inf _ { \gamma \in \Gamma ( \mu , \nu ) } \int _ { M \times M } d ( x , y ) ^ { p } \mathrm {~d} \gamma ( x , y ) \right) ^ { 1 / p }
\]</span></p>

<p>其中<span  class="math">\(\mu, \nu\)</span>为两个随机变量，<span  class="math">\(x,y\)</span>分别为<span  class="math">\(\mu, \nu\)</span>两个分布下的任意取值，<span  class="math">\(d(x,y)^p\)</span>表示<span  class="math">\(x,y\)</span>之间的距离，<span  class="math">\(p\)</span>为级数。<span  class="math">\(\gamma(x,y)\)</span>表示<span  class="math">\(\mu, \nu\)</span>的任意联合分布，其存在于<span  class="math">\(\Gamma ( \mu , \nu )\)</span>这一空间中。根据<a href="https://zhuanlan.zhihu.com/p/26988777">这篇文章</a>，Wasserstein距离可以被形象化为一个搬运石头的过程，而任意一个联合分布<span  class="math">\(\gamma(x,y)\)</span>则可以表示为一种搬运计划，<span  class="math">\(d(x,y)\)</span>表示为任意搬运起点与终点之间的距离。通过形象化的表达我们可以理解到，Wasserstein距离计算的是一种最优计划（从下确界<span  class="math">\(\inf\)</span>上可以看出来），这种计划能够通过挖土填土的方式把分布<span  class="math">\(\mu\)</span>转换成分布<span  class="math">\(\nu\)</span>。计划的优劣与搬运起点和终点之间的距离有关，搬运的距离越长，计划就越差。</p>

<p>如果<span  class="math">\(\mu,\nu\)</span>为一维分布，则Wasserstein距离可以简略为：</p>

<p><span  class="math">\[
W _ { p } ( U , Y ) = \left( \int _ { 0 } ^ { 1 } \left| F _ { Y } ^ { - 1 } ( \omega ) - F _ { U } ^ { - 1 } ( \omega ) \right| ^ { p } d \omega \right) ^ { 1 / p }
\]</span></p>

<p>其中<span  class="math">\(U,Y\)</span>为两个分布，<span  class="math">\(F _ { Y } ( y ) = \operatorname { Pr } ( Y \leq y )\)</span>为分布<span  class="math">\(Y\)</span>的概率分布函数，其接收由<span  class="math">\(Y\)</span>分布采样的一个值<span  class="math">\(y\)</span>，并给出这个分布下随机采样另一个值小于<span  class="math">\(y\)</span>的概率。而<span  class="math">\(F _ { Y } ^ { - 1 } ( \omega ) : = \inf \left\{ y \in \mathbb { R } : \omega \leq F _ { Y } ( y ) \right\}\)</span>为概率分布函数的反函数，其接收一个概率（或者分位点），返回这个分位点对应的<span  class="math">\(y\)</span>值。<span  class="math">\(p\)</span>同样表示为级数，当<span  class="math">\(p=1\)</span>时，这个公式就退化为实际上的搬运石头的距离。<span  class="math">\(W_p(U,Y)\)</span>的表达式，则是将这个分位点<span  class="math">\(\omega\)</span>从0到1进行积分。下图形象的描述了<span  class="math">\(p=1\)</span>情况下的Wasserstein距离。</p>

<p><figure><img src="/image/Wasserstein_1.png" alt=""></figure></p>

<p>上图的红线和蓝线分别为<span  class="math">\(P_X,P_Y\)</span>的概率分布函数，横轴为随机变量的值，纵轴为累积概率。对于某一个概率（分位点）<span  class="math">\(\tau\)</span>，我们可以计算得到两个值，分别为<span  class="math">\(F^{-1}_{X}(\tau)\)</span>以及<span  class="math">\(F^{-1}_{Y}(\tau)\)</span>。它们差值的绝对值就是上图中黑线的长度。把这个长度对各个分位点进行积分，就代表了两个分布的分布函数之间的差异。</p>

<p>同时对于Wasserstein距离，有几条特性。令<span  class="math">\(d_{p}(U,V)\)</span>为分布<span  class="math">\(U,V\)</span>之间的Wasserstein距离，<span  class="math">\(a\)</span>为一个标量，<span  class="math">\(A\)</span>为另一个分布。则有：</p>

<p><span  class="math">\[
\begin{aligned} d _ { p } ( a U , a V ) & \leq | a | d _ { p } ( U , V ) \\ d _ { p } ( A + U , A + V ) & \leq d _ { p } ( U , V ) \\ d _ { p } ( A U , A V ) & \leq \| A \| _ { p } d _ { p } ( U , V ) \end{aligned}
\]</span></p>

<h3 id="两个问题的说明">两个问题的说明</h3>

<p>既然介绍了Wasserstein距离，那么以下针对两个问题进行说明。第一个问题是，为什么C51算法中不能使用随机梯度下降优化作为loss的Wasserstein距离？第二个问题是，分布式的Bellman算子是否满足<span  class="math">\(\gamma - contraction\)</span>条件？</p>

<p>我们先就第一个问题进行说明。我们令<span  class="math">\(\{P_{i}\}\)</span>为随机变量的集合，其中<span  class="math">\(I \in \mathbb{N}\)</span>是指示标量。那么<span  class="math">\(P=P_I\)</span>是一个混合变量（<span  class="math">\(P\)</span>本身为随机变量集合，下标<span  class="math">\(I\)</span>又是随机的），并且所有的随机变量都与<span  class="math">\(Q\)</span>独立。令<span  class="math">\(A _ { i } = \mathbb { I } [ I = i]\)</span>（值可以为小数），则有：</p>

<p><span  class="math">\[
\begin{aligned} d _ { p } ( P , Q ) & = d _ { p } \left( P _ { I } , Q \right) \\ & = d _ { p } \left( \sum _ { i } A _ { i } P _ { i } , \sum _ { i } A _ { i } Q \right) \\ & \leq \sum _ { i } d _ { p } \left( A _ { i } P _ { i } , A _ { i } Q \right) \\ & \leq \sum _ { i } \operatorname { Pr } \{ I = i \} d _ { p } \left( P _ { i } , Q \right) \\ & = \mathbb { E } _ { I } d _ { P } \left( P _ { i } , Q \right) \end{aligned}
\]</span></p>

<p>其中第三行到第四行的转化利用了Wasserstein距离的特性3。第二行到第三行的证明其实利用了距离不会为负数的特性。我们假设<span  class="math">\(P_1 \sim N \left( -1, \sigma 1 ^ { 2 } \right)\)</span>，<span  class="math">\(P_2 \sim N \left( 1, \sigma 2 ^ { 2 } \right)\)</span>，<span  class="math">\(Q \sim N \left( 0, \sigma 3 ^ { 2 } \right)\)</span>，且<span  class="math">\(A_1=A_2=0.5\)</span>，<span  class="math">\(1/4\sigma 1 ^ { 2 } + 1/4 \sigma 2 ^ { 2 } = \sigma 3 ^ { 2 }\)</span>。可以看出，<span  class="math">\(d_p(0.5P_1, 0.5Q), d_p(0.5P_1, 0.5Q)\)</span>的距离都是大于0的，但是<span  class="math">\(\sum _ { i } A _ { i } P _ { i } = 0.5P_1 + 0.5P_2 = Q\)</span>，而<span  class="math">\(d_p(Q,Q)\)</span>则是等于0的。因为距离不会为负数，所以先把分布相加之后计算得到的距离肯定比分开计算得到的距离要短。</p>

<p>因为有：</p>

<p><span  class="math">\[
d _ { p } ( P , Q ) \leq \mathbb { E } _ { I } d _ { P } \left( P _ { i } , Q \right)
\]</span></p>

<p>所以有：</p>

<p><span  class="math">\[
\nabla _ { Q } d _ { p } \left( P _ { I } , Q \right) \neq \nabla _ { Q } \mathbb { E } _ { I } d _ { P } \left( P _ { i } , Q \right)
\]</span></p>

<p>因此无法直接使用随机梯度下降优化Wasserstein距离构成的loss。</p>

<p>现在就第二个问题进行说明。如果要证明分布式的Bellman算子满足<span  class="math">\(\gamma -contraction\)</span>条件，我们需要证明：</p>

<p><span  class="math">\[
\begin{aligned} \bar { d } _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } , \mathcal { T } ^ { \pi } Z _ { 2 } \right) \leq \gamma \bar { d } _ { p } \left( Z _ { 1 } , Z _ { 2 } \right) \end{aligned}
\]</span></p>

<p>其中<span  class="math">\(\bar { d } _ { p } \left( Z _ { 1 } , Z _ { 2 } \right) : = \sup _ { x , a } W _ { p } \left( Z _ { 1 } ( x , a ) , Z _ { 2 } ( x , a ) \right)\)</span>，对于<span  class="math">\(\bar { d } _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } , \mathcal { T } ^ { \pi } Z _ { 2 } \right)\)</span>也是一样。这里取上确界的原因是<span  class="math">\(\gamma - contraction\)</span>需要使用无穷级数，而无穷级数就相当于<span  class="math">\(\max\)</span>，也就是相当于取上确界。因为：</p>

<p><span  class="math">\[
\begin{array} { c } \bar { d } _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } , \mathcal { T } ^ { \pi } Z _ { 2 } \right) = \sup _ { x , a } d _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } ( x , a ) , \mathcal { T } ^ { \pi } Z _ { 2 } ( x , a ) \right)  \end{array}
\]</span></p>

<p>而且有：</p>

<p><span  class="math">\[
\\ d _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } ( x , a ) , \mathcal { T } ^ { \pi } Z _ { 2 } ( x , a ) \right) = d _ { p } \left( R ( x , a ) + \gamma P ^ { \pi } Z _ { 1 } ( x , a ) , R ( x , a ) + \gamma P ^ { \pi } Z _ { 2 } ( x , a ) \right) \\ \leq \gamma d _ { p } \left( P ^ { \pi } Z _ { 1 } ( x , a ) , P ^ { \pi } Z _ { 2 } ( x , a ) \right) \\ \leq \gamma \sup _ { x ^ { \prime } , a ^ { \prime } } d _ { p } \left( Z _ { 1 } \left( x ^ { \prime } , a ^ { \prime } \right) , Z _ { 2 } \left( x ^ { \prime } , a ^ { \prime } \right) \right)
\]</span></p>

<p>第一行到第二行利用了Wasserstein距离的特性1,2，第三行是因为有<span  class="math">\(P ^ { \pi } Z _ { 1 } ( x , a ) = Z_1(x',a')\)</span>，因为上确界的性质所以成立。</p>

<p>结合上述2式，有：</p>

<p><span  class="math">\[
\begin{aligned} \bar { d } _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } , \mathcal { T } ^ { \pi } Z _ { 2 } \right) & = \sup _ { x , a } d _ { p } \left( \mathcal { T } ^ { \pi } Z _ { 1 } ( x , a ) , \mathcal { T } ^ { \pi } Z _ { 2 } ( x , a ) \right) \\ & \leq \gamma \sup _ { x ^ { \prime } , a ^ { \prime } } d _ { p } \left( Z _ { 1 } \left( x ^ { \prime } , a ^ { \prime } \right) , Z _ { 2 } \left( x ^ { \prime } , a ^ { \prime } \right) \right) \\ & = \gamma \bar { d } _ { p } \left( Z _ { 1 } , Z _ { 2 } \right) \end{aligned}
\]</span></p>

<p>所以能够满足<span  class="math">\(\gamma - contraction\)</span>条件。</p>

<h3 id="近似最小化wasserstein距离">近似最小化Wasserstein距离</h3>

<p>根据前面对两个问题的说明，我们知道无法直接使用Wasserstein距离进行梯度下降优化，因此QR-DQN算法使用了近似方法。根据Wasserstein距离一节的说明可以知道，当在一维分布且<span  class="math">\(p=1\)</span>的情况下，Wasserstein距离其实就相当于两个分布的分布函数之间相差的面积。因此QR-DQN算法放弃考虑概率密度函数，而转向考虑概率分布函数。它通过锁定概率分布函数上各个指定分位点的值来概括这个分布，并使用分位数回归的方法来进行拟合。</p>

<p>在C51算法中，我们使用<span  class="math">\(N\)</span>个atoms<span  class="math">\(\{z_0, z_1, \cdots , z_{N-1}\}\)</span>作为基准，再用<span  class="math">\(N\)</span>个离散的分布<span  class="math">\(\{p_0, p_1, \cdots, p_{N-1}\}\)</span>来描述这个分布。这种形式不太适合Wasserstein距离，但是我们可以使用相似的方法。</p>

<p>我们可以把概率分布函数的<span  class="math">\(y\)</span>轴均等的分成<span  class="math">\(N\)</span>等分，例如下面分成了10等分：（图源<a href="https://zhuanlan.zhihu.com/p/138091493">这里</a>)</p>

<p><figure><img src="/image/Quantile.jpg" alt=""></figure></p>

<p>我们取10个区间的中点作为记录点，得到10个分位数。可以表示为：</p>

<p><span  class="math">\[
\hat { \tau } _ { i } = \frac { 2 ( i - 1 ) + 1 } { 2 N } , \quad i = 1 , \ldots , N
\]</span></p>

<p>使用中点作为记录点是有原因的，具体可以看<a href="https://zhuanlan.zhihu.com/p/272228531">这篇文章</a>的3.3部分。</p>

<p>使用概率密度函数来可视化的话就像这样：</p>

<p><figure><img src="/image/Quantile_density.jpg" alt=""></figure></p>

<h3 id="分位数回归">分位数回归</h3>

<p>具体可以参照<a href="https://zhuanlan.zhihu.com/p/40681570">这篇文章</a>。</p>

<p>在传统的回归中，对于自变量<span  class="math">\(x\)</span>，我们想要建立方程<span  class="math">\(\mu(x,\beta)\)</span>拟合因变量<span  class="math">\(y\)</span>的期望，我们使用的损失函数为：</p>

<p><span  class="math">\[
L _ { M S E } = \min _ { \beta } \sum _ { i } ^ { n } \left( y _ { i } - \mu \left( x _ { i } , \beta \right) \right) ^ { 2 }
\]</span></p>

<p>但如果我们想求<span  class="math">\(y\)</span>的中位数，那我们就应该把损失函数换成：</p>

<p><span  class="math">\[
L _ { M A E } = \sum _ { i } ^ { n } \left| y _ { i } - \xi \left( x _ { i } , \beta \right) \right|
\]</span></p>

<p>或者写成：</p>

<p><span  class="math">\[
L _ { M A E } = \sum _ { i : y _ { i } \geq \xi \left( x _ { i } , \beta \right) } \left( y _ { i } - \xi \left( x _ { i } , \beta _ { \tau } \right) \right) + \sum _ { i : y _ { i } < \xi \left( x _ { i } , \beta \right) } \left( \xi \left( x _ { i } , \beta \right) - y _ { i } \right)
\]</span></p>

<p>至于为什么，因为这个函数当<span  class="math">\(\xi  (x _ { i } , \beta ) \)</span>预测为中位数时导数等于0，导数为：</p>

<p><span  class="math">\[
\begin{array} { l } \nabla _ { \beta } L _ { M A E } = \sum _ { i } ^ { n } \nabla _ { i } \\  \nabla _ { i } = \left\{ \begin{array} { l l } 1 & i : y _ { i } < \xi \left( x _ { i } , \beta \right) \\ - 1 & i : y _ { i } \geq \xi \left( x _ { i } , \beta \right) \end{array} \right. \end{array}
\]</span></p>

<p>对于其他分位数，我们可以给损失函数的两项分配不同的权重，假设我们要求<span  class="math">\(\tau\)</span>分位数，则损失函数可以写为：</p>

<p><span  class="math">\[
L _ { \tau } = \sum _ { i : y _ { i } \geq \xi \left( x _ { i } , \beta _ { \tau } \right) } \tau \left( y _ { i } - \xi \left( x _ { i } , \beta _ { \tau } \right) \right) + \sum _ { i : y _ { i } < \xi \left( x _ { i } , \beta _ { \tau } \right) } ( 1 - \tau ) \left( \xi \left( x _ { i } , \beta _ { \tau } \right) - y _ { i } \right)
\]</span></p>

<p>简单描述就是给<span  class="math">\(\tau\)</span>分位数左边的样本分配<span  class="math">\((1-\tau)\)</span>的权重，对右边的样本分配<span  class="math">\(\tau\)</span>的权重。这样当导数为0的时候方程刚好能够正确预测<span  class="math">\(\tau\)</span>分位数的值。这个时候的导数可以表示为：</p>

<p><span  class="math">\[
\begin{array} { l } \nabla _ { \beta } L _ { \tau } = \sum _ { i } ^ { n } \nabla _ { i } \\  \nabla _ { i } = \left\{ \begin{array} { l l } 1 - \tau & i : y _ { i } < \xi \left( x _ { i } , \beta _ { \tau } \right) \\ - \tau & i : y _ { i } \geq \xi \left( x _ { i } , \beta _ { \tau } \right) \end{array} \right. \end{array}
\]</span></p>

<p>我们还可以换一种形式表示<span  class="math">\(L_\tau\)</span>，令<span  class="math">\(\delta_{\text{statement}}\)</span>表示：</p>

<p><span  class="math">\[
\delta _ { \text {statement } } = \left\{ \begin{array} { l l } 1 & \text { statement is ture } \\ 0 & \text { otherwise } \end{array} \right.
\]</span></p>

<p>于是有：</p>

<p><span  class="math">\[
\left| \tau - \delta _ { y _ { i } < \xi \left( x _ { i } , \beta \right) } \right| = \left\{ \begin{array} { l l } | \tau - 1 | = 1 - \tau & \text { if } y _ { i } < \xi \left( x _ { i } , \beta \right) \\ \tau & \text { if } y _ { i } \geq \xi \left( x _ { i } , \beta \right) \geq \theta \end{array} \right.
\]</span></p>

<p>所以</p>

<p><span  class="math">\[
\begin{aligned} L _ { \tau } & = \mathbb { E } \left[ \left| y _ { i } - \xi \left( x _ { i } , \beta _ { \tau } \right) \right| \left| \tau - \delta _ { y _ { i } < \xi \left( x _ { i } , \beta \right) } \right| \right] \\ & = \mathbb { E } \left[ \rho _ { \tau } \left( y _ { i } - \xi \left( x _ { i } , \beta _ { \tau } \right) \right) \right] \end{aligned}
\]</span></p>

<p>因为两个绝对值内的式子都在同时正同时负，所以抵消了。其中<span  class="math">\(\rho_{\tau}(u) = u(\tau - \delta_{u <0})\)</span>。</p>

<p>因为绝对值<span  class="math">\(|u|\)</span>在0处不可导，所以论文中使用的是近似的函数：</p>

<p><span  class="math">\[
\mathcal { L } _ { \kappa } ( u ) = \left\{ \begin{array} { l l } \frac { 1 } { 2 } u ^ { 2 } & \text { if } | u | \leq \kappa \\ \kappa \left( | u | - \frac { 1 } { 2 } \kappa \right) & \text { otherwise } \end{array} \right.
\]</span></p>

<p>其中<span  class="math">\(\kappa\)</span>也是超参数，一般取1，这时的图像如下：</p>

<p><figure><img src="/image/huber.jpg" alt=""></figure></p>

<p>最终我们把损失函数定为<span  class="math">\(\mathbb { E } \left[ \rho^{1} _ { \tau } \left( y _ { i } - \xi \left( x _ { i } , \beta _ { \tau } \right) \right) \right] \)</span>，其中<span  class="math">\(\tau\)</span>表示分位数，1表示<span  class="math">\(\kappa\)</span>的值。</p>

<p>其实使用分位数回归来近似Wasserstein距离的损失函数也是需要证明的，证明过程可以参照<a href="https://zhuanlan.zhihu.com/p/272228531">这篇文章</a>的3.3部分。</p>

<h3 id="训练过程">训练过程</h3>

<p>首先神经网络接收状态<span  class="math">\(s\)</span>，并输出<span  class="math">\(|\mathcal{A}| \times N\)</span>的矩阵，其中每一行<span  class="math">\(\{\theta_0, \theta_1, \cdots, \theta_{N-1}\}\)</span>表示各个分位数对应的<span  class="math">\(Q(s,a)\)</span>值。</p>

<p>同样是使用类q-learning的思想。采样一个<span  class="math">\((s,a,r,s')\)</span>，接下来我们需要计算出<span  class="math">\(a^*\)</span>，一样用<span  class="math">\(Q(s,a)\)</span>来计算。有：</p>

<p><span  class="math">\[
Q(s',a') = \sum_{j}q_j \theta_j(x',a')
\]</span></p>

<p>其中<span  class="math">\(q_j = 1/N\)</span>。根据<span  class="math">\(Q(s',a')\)</span>挑出最大的<span  class="math">\(a^*\)</span>，设其分布为<span  class="math">\(\{\theta'_0, \theta'_1, \cdots, \theta'_{N-1}\}\)</span>，那么目标分布表示为：</p>

<p><span  class="math">\[
\mathcal { T } \theta _ { j } ^ { \prime } = r + \gamma \theta _ { j } ^ { \prime } , \quad i = 0 , \ldots , N - 1
\]</span></p>

<p>所以现在有当前分布<span  class="math">\(Z(s,a) = \{\theta_0, \theta_1, \cdots, \theta_{N-1}\}\)</span>，目标分布<span  class="math">\(\{r+\gamma\theta'_0, r+\gamma\theta'_1, \cdots, r+\gamma\theta'_{N-1}\}\)</span>，使用损失函数<span  class="math">\(L_\tau = \mathbb { E } \left[ \rho _ { \tau } \left( y _ { i } - \xi \left( x _ { i } , \beta _ { \tau } \right) \right) \right]\)</span>计算loss有：</p>

<p><span  class="math">\[
\begin{aligned} L _ { \beta } & = \sum _ { i = 1 } ^ { N } \mathbb { E } _ { Y } \left[ \rho _ { \tau _ { i } } ^ { 1 } \left( Y - \xi ( \beta ) _ { i } \right) \right] \\ & = \sum _ { i = 1 } ^ { N } \mathbb { E } _ { \mathcal { T } Z ^ { \prime } } \left[ \rho _ { \hat { \tau } _ { i } } ^ { 1 } \left( \mathcal { T } Z ^ { \prime } - \theta _ { i } \right) \right] \\ & = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { N } \left[ \rho _ { \hat { \tau } _ { i } } ^ { 1 } \left( \mathcal { T } \theta _ { j } ^ { \prime } - \theta _ { i } \right) \right] \end{aligned}
\]</span></p>

<p>注意每一个<span  class="math">\(\theta_j\)</span>都要与所有的<span  class="math">\(r+\gamma\theta'_i\)</span>计算loss（根据分位数回归方程，不需要对<span  class="math">\(y\)</span>进行筛选）,因此需要计算两次求和。上式中，有：</p>

<p><span  class="math">\[
\mathcal{T}Z' = r + \gamma Z(x',a^*) 
\]</span></p>

<p>且<span  class="math">\(\hat { \tau } _ { i }\)</span>就是用来决定<span  class="math">\(N\)</span>个分位数的值：</p>

<p><span  class="math">\[
\hat { \tau } _ { i } = \frac { 2 i + 1 } { 2 N } , \quad i = 0 , \ldots , N - 1
\]</span></p>

<p>最后算法如下：</p>

<p><figure><img src="/image/QR-DQN.png" alt=""></figure></p>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/gpg%E5%8A%A0%E5%AF%86/"><span class="li">&larr;</span>GPG加密</a></p><p><a class="link" href="/post/c51/"><span class="li">&rarr;</span>C51</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>