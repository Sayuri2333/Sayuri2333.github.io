<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Double DQN算法&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">Double DQN算法</h1><p class="article date">Thursday, October 29, 2020</p></section><article class="article markdown-body"><h3 id="dqn-以及其改进">DQN 以及其改进</h3>

<p>在原版Q-learning算法中，Q网络的优化目标为：</p>

<p><span  class="math">\[
Y_{t}^{\mathrm{Q}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right)
\]</span></p>

<p>基于此优化目标建立均方误差损失函数，优化迭代式如下：</p>

<p><span  class="math">\[
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha\left(Y_{t}^{\mathrm{Q}}-Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right)\right) \nabla_{\boldsymbol{\theta}_{t}} Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right)
\]</span></p>

<p>与原版（2013年）发布的Deep Q-learning算法相比，与2015年发布的Deep Q-learning算法对原版算法进行了改进。2015年的改进版本DQN有两个较为重要的特点：目标网络的使用以及经验回放池的引入。目标网络与原版网络具有相同的结构，令原版网络的参数为<span  class="math">\(\theta_{t}\)</span>，目标网络的参数为<span  class="math">\(\theta_{t}^{-}\)</span>，则改进后的DQN的优化目标为：</p>

<p><span  class="math">\[
Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}^{-}\right)
\]</span></p>

<p>其中目标网络每<span  class="math">\(\tau\)</span>步从原网络复制参数，并在剩下的时间步中保持不变。</p>

<h3 id="dqn算法的缺陷">DQN算法的缺陷</h3>

<p>上述两种Q网络的优化目标都存在一定缺陷，即使用同一个网络的value来选择和评价action。在<span  class="math">\(Y_{t}^{\mathrm{Q}}\)</span>中，使用原始Q网络根据max策略选择action，并使用同一个网络的值对此action进行评价；在<span  class="math">\(Y_{t}^{\mathrm{DQN}}\)</span>中进行的是相同的操作，只是其发生在目标Q网络中。这种情况下通常会选择过估计的value值，从而对整个网络的Q值进行过分乐观估计。为了解决上述问题，Double DQN对action选择以及评估进行了解耦，使用当前网络选择最优action，并使用目标网络计算对应value。改进后的优化目标为：</p>

<p><span  class="math">\[
Y_{t}^{\text {DoubleQ }} \equiv R_{t+1}+\gamma Q\left(S_{t+1}, \operatorname{argmax} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) ; \boldsymbol{\theta}_{t}^{\prime}\right)
\]</span></p>

<h3 id="过优估计">过优估计</h3>

<p>以下为结合知乎回答<a href="https://www.zhihu.com/question/394866647/answer/1265905876">1</a>以及<a href="https://www.zhihu.com/question/67758340/answer/256406504">2</a>的个人理解。</p>

<p>首先我们有公式：</p>

<p><span  class="math">\[
E\left(\max \left(y_{1}, y_{2}, \ldots\right)\right) \geq \max \left(E\left(y_{1}, y_{2}, \ldots\right)\right)
\]</span></p>

<p>直观描述为对一系列数求最大值再求平均通常比先求平均值再求最大值要大。</p>

<p>由于Q-learning算法通常以拟合Bellman最优函数为目标：</p>

<p><span  class="math">\[
Q_{\pi}(s, a)=R(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim p(s, a)}\left[\max _{a^{\prime}} Q_{\pi}\left(s^{\prime}, a^{\prime}\right)\right]
\]</span></p>

<p>但理想情况下<span  class="math">\(Q_{\pi}(s, a)\)</span>的值应该等同于下式：</p>

<p><span  class="math">\[
Q_{\pi}(s, a) = \mathbb{E}(R(s, a)) + \gamma \mathbb{E}_{s^{\prime} \sim p(s, a)}\left[\max_{a^{\prime} \sim \pi} \mathbb{E}[Q_{\pi}\left(s^{\prime}, a^{\prime}\right)]\right]
\]</span></p>

<p>即理想情况下应该先求出对于可能出现的状态<span  class="math">\(s^{\prime}\)</span>以及可能的动作<span  class="math">\(a^{\prime}\)</span>的Q值的期望，再对期望求最大值并根据<span  class="math">\(s^{\prime}\)</span>出现的概率求总期望。根据上面的不等式有：</p>

<p><span  class="math">\[
\mathbb{E}[\max _{a^{\prime}} Q_{\pi}\left(s^{\prime}, a^{\prime}\right)] \geq \max_{a^{\prime} \sim \pi} \mathbb{E}[Q_{\pi}\left(s^{\prime}, a^{\prime}\right)]
\]</span></p>

<p>因此Q-learning对于最优函数的拟合总会出现乐观估计。</p>

<h3 id="算法流程">算法流程</h3>

<p>参考<a href="https://www.cnblogs.com/pinard/p/9778063.html">Pinard的博客</a>。</p>

<p>具体Python实现参考<a href="https://github.com/Sayuri2333/reinforcement_learning">我的github</a>。</p>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/per/"><span class="li">&larr;</span>Prioritized Experience Replay</a></p><p><a class="link" href="/post/ddpg%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98/"><span class="li">&rarr;</span>重要性采样的问题</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>