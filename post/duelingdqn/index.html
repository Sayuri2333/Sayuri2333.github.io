<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Dueling DQN算法&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">Dueling DQN算法</h1><p class="article date">Friday, October 30, 2020</p></section><article class="article markdown-body"><h3 id="dueling-dqn的改进">Dueling DQN的改进</h3>

<p>Dueling DQN算法主要针对传统DQN算法对于状态价值的评估做出了改进。在传统的DQN算法中，Q网络能够预测给定<span  class="math">\((s,a)\)</span>的状态价值，但是却无法针对状态本身的价值做出评估。这样的评估在某些无论做出任何动作都无法有效影响价值的时候相当有用。同时因为Dueling DQN算法将value function以及advantage function分割，如果网络中新增新的动作这种结构也能很快适应。由于Dueling DQN算法只是针对网络结构做出调整，因此也能与其他方法共同使用。</p>

<h3 id="dueling-dqn算法说明">Dueling DQN算法说明</h3>

<p>Dueling DQN算法引入了优势函数（advantage function）的概念。对于定义：</p>

<p><span  class="math">\[
Q^{\pi}(s, a)=\mathbb{E}\left[R_{t} \mid s_{t}=s, a_{t}=a, \pi\right]
\]</span></p>

<p><span  class="math">\[
V^{\pi}(s)=\mathbb{E}_{a \sim \pi(s)}\left[Q^{\pi}(s, a)\right]
\]</span></p>

<p>我们设置优势函数为：</p>

<p><span  class="math">\[
A^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s)
\]</span></p>

<p>通过使用Q function减去value function，优势函数能够衡量某个状态下指定动作对局面影响的方向和大小。同时对于优势函数，有：</p>

<p><span  class="math">\[
\mathbb{E}_{a \sim \pi(s)}\left[A^{\pi}(s, a)\right]=0
\]</span></p>

<p>通过引入优势函数，Dueling DQN算法将Q网络的输出改为了以下公式：</p>

<p><span  class="math">\[
Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+A(s, a ; \theta, \alpha)
\]</span></p>

<p>但是上述公式存在一个问题，那就是缺乏可辨识性（identifiability），也就是说给定一个Q值，我们无法分解出唯一的V值和A值。 举个例子，把一个常量加到V中，并且从A中减去该常量，那么Dueling DQN仍输出相同的Q值。这个unidentifiable问题会严重地降低网络性能。也就是说，V不能反映state值，A不能反映advantage值。</p>

<p>为了解决这个问题，我们可以尝试将<span  class="math">\(Q(s, a* ; \theta, \alpha, \beta)\)</span>与<span  class="math">\(V(s ; \theta, \beta)\)</span>对齐，其中<span  class="math">\(a*\)</span>为最优动作。即使得：</p>

<p><span  class="math">\[
Q(s, a* ; \theta, \alpha, \beta) = V(s ; \theta, \beta)
\]</span></p>

<p>为了达成上述对齐方式，我们可以令</p>

<p><span  class="math">\[
\begin{array}{l}
Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+\quad\left(A(s, a ; \theta, \alpha)-\max _{a^{\prime} \in|\mathcal{A}|} A\left(s, a^{\prime} ; \theta, \alpha\right)\right)
\end{array}
\]</span></p>

<p>使得当取得最优动作时优势函数部分为0。</p>

<p>但是由于max函数不好求导，在优化时不够平滑，因此论文中提出了下式改进：</p>

<p><span  class="math">\[
\begin{array}{l}
Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+\quad\left(A(s, a ; \theta, \alpha)-\frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A\left(s, a^{\prime} ; \theta, \alpha\right)\right)
\end{array}
\]</span></p>

<p>虽然此时V和A的值都有偏差，但是相对于减去最大值，减去期望的方式能使得优化更为平滑。</p>

<h3 id="dueling-dqn算法流程">Dueling DQN算法流程</h3>

<p>基本同Double DQN算法，只是改变了网络结构和优化目标。</p>

<p>具体Python实现参考<a href="https://github.com/Sayuri2333/reinforcement_learning">我的GitHub</a>。</p>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/solve_project/"><span class="li">&larr;</span>Wine Quality Prediction With Random Forest</a></p><p><a class="link" href="/post/per/"><span class="li">&rarr;</span>Prioritized Experience Replay</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>