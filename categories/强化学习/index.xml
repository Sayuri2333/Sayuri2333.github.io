<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习 on SayuriBlog</title>
    <link>https://sayuri2333.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习 on SayuriBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2020 Sayuri2333.</copyright>
    <lastBuildDate>Fri, 05 Mar 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://sayuri2333.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>QR-DQN</title>
      <link>https://sayuri2333.github.io/post/qr-dqn/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/qr-dqn/</guid>
      <description>C51算法的缺陷 C51算法中虽然证明了\(\mathcal{T}\)Bellman算子在使用分布时是满足\(\gamma -contracti</description>
    </item>
    
    <item>
      <title>C51</title>
      <link>https://sayuri2333.github.io/post/c51/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/c51/</guid>
      <description>经典强化学习与值分布强化学习的区别 由于状态转移的随机性，状态表示的混叠效应（编码状态时带来的信息丢失）以及函数逼近的引入（使用函数表示状态价</description>
    </item>
    
    <item>
      <title>DDPG</title>
      <link>https://sayuri2333.github.io/post/ddpg/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/ddpg/</guid>
      <description>DDPG的改进 DDPG可以被视为一个对传统PG算法的改进。在Policy Gradient一文中，我们提到了REINFORCE算法，它实际上与</description>
    </item>
    
    <item>
      <title>Actor-Critic</title>
      <link>https://sayuri2333.github.io/post/actor-critic/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/actor-critic/</guid>
      <description>Policy Gradient的改进 在Policy Gradient的REINFORCE算法实现中，如果某个状态下采样到的复数个动作都取得了正的奖励时，那</description>
    </item>
    
    <item>
      <title>Noisy Network</title>
      <link>https://sayuri2333.github.io/post/noisynet/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/noisynet/</guid>
      <description>背景 对于DRL中面对的exploration-exploition问题，目前广泛使用以下两种方法： ε-greedy法 通常应用于DQN类算法中</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://sayuri2333.github.io/post/policy-gradient/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/policy-gradient/</guid>
      <description>DQN类算法的不足 无法表示随机策略。 输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。 无法表示</description>
    </item>
    
    <item>
      <title>Deep Recurrent Q Network</title>
      <link>https://sayuri2333.github.io/post/drqn/</link>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/drqn/</guid>
      <description>背景 在DQN的原文中，需要将游戏最近4帧的图像作为Q网络的输入。这是因为仅仅凭借1帧的画面无法判断物体运动速度和方向等的相关信息。但是在某些</description>
    </item>
    
    <item>
      <title>Wine Quality Prediction With Random Forest</title>
      <link>https://sayuri2333.github.io/post/solve_project/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/solve_project/</guid>
      <description>Introduction For winemakers, it is very important to know how to judge the quality of wine by its chemical components. In this report, we analyze the white wine dataset, use random forest algorithm and logistic regression algorithm to build models to distinguish the quality of wine, and determine the importance of each chemical component for wine quality judgment by its weights in both algorithm. Exploratory Data Analysis First import the</description>
    </item>
    
    <item>
      <title>Dueling DQN算法</title>
      <link>https://sayuri2333.github.io/post/duelingdqn/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/duelingdqn/</guid>
      <description>Dueling DQN的改进 Dueling DQN算法主要针对传统DQN算法对于状态价值的评估做出了改进。在传统的DQN算法中，Q网络能够预测给定\((s,a)\)的状</description>
    </item>
    
    <item>
      <title>Prioritized Experience Replay</title>
      <link>https://sayuri2333.github.io/post/per/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://sayuri2333.github.io/post/per/</guid>
      <description>PER的由来 在PER之前，像DQN（Nature 2015）以及Double DQN等Deep Q-learning方法都是通过经验回放的手段进行</description>
    </item>
    
  </channel>
</rss>