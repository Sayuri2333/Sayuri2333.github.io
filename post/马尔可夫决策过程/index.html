<!DOCTYPE html>
<html lang="zh-cn">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#494f5c">
	<meta name="msapplication-TileColor" content="#494f5c">
<meta itemprop="name" content="马尔可夫决策过程">
<meta itemprop="description" content="引入马尔可夫决策过程(MDP) 在强化学习基础介绍的强化学习要素中，状态转移概率\(P_{ss&#39;}^a\)不仅与上一个状态有关，也与之前所有状">
<meta itemprop="datePublished" content="2020-05-06T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-05-06T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1068">



<meta itemprop="keywords" content="" /><meta property="og:title" content="马尔可夫决策过程" />
<meta property="og:description" content="引入马尔可夫决策过程(MDP) 在强化学习基础介绍的强化学习要素中，状态转移概率\(P_{ss&#39;}^a\)不仅与上一个状态有关，也与之前所有状" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/post/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" />
<meta property="article:published_time" content="2020-05-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-06T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="马尔可夫决策过程"/>
<meta name="twitter:description" content="引入马尔可夫决策过程(MDP) 在强化学习基础介绍的强化学习要素中，状态转移概率\(P_{ss&#39;}^a\)不仅与上一个状态有关，也与之前所有状"/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>马尔可夫决策过程</title>
	<link rel="stylesheet" href="https://example.com/css/style.min.eac77496566fd7d5768fd650ddb0b2b181ca6a2d7c5fdd6fe6b8ba4bf47e566f.css" integrity="sha256-6sd0llZv19V2j9ZQ3bCysYHKai18X91v5ri6S/R+Vm8=" crossorigin="anonymous">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://example.com">SayuriBlog</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://example.com/post/">Posts</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-social hide-in-mobile"><a href="https://github.com/Sayuri2333" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://example.com/post/">Posts</a></li>
		</ul>
	</div>


	<main class="site-main section-inner thin animated fadeIn faster">
		<h1>马尔可夫决策过程</h1>
		<div class="content">
			<h3 id="引入马尔可夫决策过程mdp">引入马尔可夫决策过程(MDP)<a href="#引入马尔可夫决策过程mdp" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>在强化学习基础介绍的强化学习要素中，状态转移概率<span  class="math">\(P_{ss'}^a\)</span>不仅与上一个状态有关，也与之前所有状态形成的状态序列有关。为了减少建模的难度，我们假设状态的转移具有马尔可夫性，即转移到下一个状态的概率仅仅与上一个状态<span  class="math">\(s\)</span>有关，与以前的状态序列无关。以上假设可以使用公式表示：</p>

<p><span  class="math">\[
P_{ss'}^a = \mathbb{E}(S_{t+1}=s'|S_t=s, A_t=a)
\]</span></p>

<p>除此之外，我们也对强化学习中的策略<span  class="math">\(\pi\)</span>进行了马尔可夫性的假设。即在状态<span  class="math">\(s\)</span>下采取的动作仅仅与当前状态有关，而与以前的状态序列无关。使用公式表示为：</p>

<p><span  class="math">\[
\pi(a|s) = P(A_t=a | S_t=s)
\]</span></p>

<p>同样的，对于价值函数<span  class="math">\(v_{\pi}(s)\)</span>也有同样的操作：</p>

<p><span  class="math">\[
v_{\pi}(s) = \mathbb{E}_{\pi}(G_t|S_t=s ) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+...|S_t=s)
\]</span></p>

<p>其中<span  class="math">\(G_t\)</span>代表在状态<span  class="math">\(s\)</span>下的reward，它是一个马尔可夫过程中从状态<span  class="math">\(s\)</span>开始采样直到终止状态时所有即时奖励的有衰减的和。</p>

<h3 id="mdp的价值函数与贝尔曼方程">MDP的价值函数与贝尔曼方程<a href="#mdp的价值函数与贝尔曼方程" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>MDP中除了状态价值函数<span  class="math">\(v_{\pi}(s)\)</span>以外，还有动作价值函数<span  class="math">\(q_{\pi}(s,a)\)</span>,其表示的是动作<span  class="math">\(a\)</span>以及状态<span  class="math">\(s\)</span>联合对价值带来的影响。其计算公式为：</p>

<p><span  class="math">\[
q_{\pi}(s,a) = \mathbb{E}_{\pi}(G_t|S_t=s, A_t=a) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2}+...|S_t=s,A_t=a)
\]</span></p>

<p>根据状态价值函数的表达式，我们可以得出状态价值函数的递推式：</p>

<p><span  class="math">\[
v_{\pi}(s) = \mathbb{E}_{\pi}(R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s)
\]</span></p>

<p>这条方程被称为贝尔曼方程。它表示当前状态的价值可以由该状态的奖励以及后续状态的价值按照一定的衰减比例计算而成。</p>

<p>同样的方法，我们可以得到动作价值函数的贝尔曼方程:</p>

<p><span  class="math">\[
q_{\pi}(s,a) = \mathbb{E}_{\pi}(R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) | S_t=s, A_t=a)
\]</span></p>

<h3 id="状态价值函数与动作价值函数的递推关系">状态价值函数与动作价值函数的递推关系<a href="#状态价值函数与动作价值函数的递推关系" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>根据动作价值函数以及状态价值函数的定义，可以得到他们的转化关系公式：</p>

<p><span  class="math">\[
v_{\pi}(s) = \sum\limits_{a \in A} \pi(a|s)q_{\pi}(s,a)
\]</span></p>

<p>即状态价值函数是动作价值函数基于给定策略<span  class="math">\(\pi\)</span>的期望。</p>

<p>反过来，我们也可以使用状态价值函数表示动作价值函数：</p>

<p><span  class="math">\[
q_{\pi}(s,a) = R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{\pi}(s')
\]</span></p>

<p>即动作价值函数是即时奖励加上当前动作可能转移到的下一个状态的状态价值函数基于状态转移概率的有衰减的和。</p>

<p>根据以上公式，我们可以分别得到动作价值函数以及状态价值函数的递推式：</p>

<p><span  class="math">\[
v_{\pi}(s) = \sum\limits_{a \in A} \pi(a|s)(R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{\pi}(s'))
\]</span></p>

<p><span  class="math">\[
q_{\pi}(s,a) = R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^a\sum\limits_{a' \in A} \pi(a'|s')q_{\pi}(s',a')
\]</span></p>

<h3 id="最优价值函数">最优价值函数<a href="#最优价值函数" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>寻找最优策略可以通过最优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者。即<span  class="math">\(v_{*}(s) = \max_{\pi}v_{\pi}(s)\)</span>。</p>

<p>同理也可以定义最优动作价值函数是所有策略下产生的众多动作价值函数中的最大值：</p>

<p><span  class="math">\[
q_{*}(s,a) = \max_{\pi}q_{\pi}(s,a)
\]</span></p>

<p>对于最优的策略，基于动作价值函数我们可以定义为：</p>

<p><span  class="math">\[
\pi_{*}(a|s)= \begin{cases} 1 & {a=\arg\max_{a \in A}q_{*}(s,a)}\\ 0 & {else} \end{cases}
\]</span></p>

<p>此时利用状态价值函数与动作价值函数之间的关系，也可以得到：</p>

<p><span  class="math">\[
v_{*}(s) = \max_{a}q_{*}(s,a)
\]</span></p>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2020 <a href="https://example.com"></a> Sayuri2333 &#183; <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://example.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
	</footer>



	<script src="https://example.com/js/bundle.min.4a9a0ac3d2217822c7865b4161e6c2a71de1d70492264337755427898dd718f6.js" integrity="sha256-SpoKw9IheCLHhltBYebCpx3h1wSSJkM3dVQniY3XGPY=" crossorigin="anonymous"></script>
	

</body>

</html>
