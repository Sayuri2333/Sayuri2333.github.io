<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>强化学习&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><link rel="alternate" type="application/rss+xml" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" title="SayuriBlog" /><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header"><h1>强化学习</h1></section><ul class="note list"><li class="item"><a class="note" href="/post/doubledqn/">
            <p class="note title">Double DQN算法</p><p class="note date">Thursday, October 29, 2020</p><p class="note content">DQN 以及其改进 在原版Q-learning算法中，Q网络的优化目标为： \[ Y_{t}^{\mathrm{Q}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) \] 基于此优化目标建立均方误差损失函数，优化迭代式如下：<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/ddpg%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98/">
            <p class="note title">重要性采样的问题</p><p class="note date">Thursday, October 29, 2020</p><p class="note content">重要性采样在策略梯度类算法的应用 PPO算法是重要性采样在策略梯度类算法中应用的典型成果。策略梯度类算法通常以最大化期望奖励（Expected<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/deep-q-learning/">
            <p class="note title">Deep Q-learning算法</p><p class="note date">Friday, May 8, 2020</p><p class="note content">价值函数的近似表示 之前介绍的强化学习方法使用的都是有限个状态集合\(S\)，而当遇到连续的状态时，则需要价值函数的近似表示。 价值函数的近似表<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/q-learning/">
            <p class="note title">Q-learning算法</p><p class="note date">Friday, May 8, 2020</p><p class="note content">Q-learning算法的引入 Q-learning算法的步骤与SARSA算法大致相同，唯一不同的地方在于SARSA算法在更新价值函数时会使用<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/sarsa/">
            <p class="note title">SARSA算法</p><p class="note date">Friday, May 8, 2020</p><p class="note content">SARSA算法的引入 SARSA算法不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题的求解与蒙特卡洛法相似，即通<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/">
            <p class="note title">时序差分法求解</p><p class="note date">Thursday, May 7, 2020</p><p class="note content">时序差分法简介 时序差分法与蒙特卡洛法相似，都是不基于模型的强化学习问题求解方法。时序差分法使用不完整的状态序列近似求出给定状态的收获。回顾贝<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95/">
            <p class="note title">蒙特卡洛法求解</p><p class="note date">Thursday, May 7, 2020</p><p class="note content">不基于模型的强化学习问题定义 在动态规划一文中，我们提到强化学习的两个基本问题：预测问题与控制问题。在处理上述两种问题的时候，其状态转化概率矩<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">
            <p class="note title">动态规划求解强化学习问题</p><p class="note date">Wednesday, May 6, 2020</p><p class="note content">动态规划与强化学习的关系 动态规划的关键点在于：（1）问题的最优解可以由若干个小问题的最优解构成，即通过寻找子问题的最优解可以得到问题的最优解<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">
            <p class="note title">强化学习基础</p><p class="note date">Wednesday, May 6, 2020</p><p class="note content">强化学习基本要素 环境状态\(S\), t时刻环境的状态\(S_t\)是它的环境状态集中的某一个状态。 个体动作\(A\)， t时刻个体采取的动作\<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/">
            <p class="note title">马尔可夫决策过程</p><p class="note date">Wednesday, May 6, 2020</p><p class="note content">引入马尔可夫决策过程(MDP) 在强化学习基础介绍的强化学习要素中，状态转移概率\(P_{ss'}^a\)不仅与上一个状态有关，也与之前所有状<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li></ul><div class="pagination">
    <ul><li><a class="" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">1</a></li><li><a class="active" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/page/2/">2</a></li><li><a class="" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/page/3/">3</a></li></ul>
</div></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>