<!DOCTYPE html>
<html lang="zh-cn">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#494f5c">
	<meta name="msapplication-TileColor" content="#494f5c">
<meta itemprop="name" content="Policy Gradient">
<meta itemprop="description" content="DQN类算法的不足 无法表示随机策略。 输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。 无法表示">
<meta itemprop="datePublished" content="2021-03-02T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2021-03-02T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1424">



<meta itemprop="keywords" content="" /><meta property="og:title" content="Policy Gradient" />
<meta property="og:description" content="DQN类算法的不足 无法表示随机策略。 输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。 无法表示" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sayuri2333.github.io/post/policy-gradient/" />
<meta property="article:published_time" content="2021-03-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-02T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Policy Gradient"/>
<meta name="twitter:description" content="DQN类算法的不足 无法表示随机策略。 输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。 无法表示"/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>Policy Gradient</title>
	<link rel="stylesheet" href="https://sayuri2333.github.io/css/style.min.eac77496566fd7d5768fd650ddb0b2b181ca6a2d7c5fdd6fe6b8ba4bf47e566f.css" integrity="sha256-6sd0llZv19V2j9ZQ3bCysYHKai18X91v5ri6S/R+Vm8=" crossorigin="anonymous">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://sayuri2333.github.io/">SayuriBlog</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://sayuri2333.github.io/post/">Posts</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-social hide-in-mobile"><a href="https://github.com/Sayuri2333" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://sayuri2333.github.io/post/">Posts</a></li>
		</ul>
	</div>


	<main class="site-main section-inner thin animated fadeIn faster">
		<h1>Policy Gradient</h1>
		<div class="content">
			<h3 id="dqn类算法的不足">DQN类算法的不足<a href="#dqn类算法的不足" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<ol>
<li>无法表示随机策略。</li>
<li>输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。</li>
<li>无法表示连续动作。</li>
</ol>

<h3 id="策略梯度算法">策略梯度算法<a href="#策略梯度算法" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>相比于DQN类算法使用参数化函数表示价值函数，策略梯度算法则使用参数化函数表示策略。令<span  class="math">\(\theta \in \mathbb{R}^{d}\)</span>表示参数向量，则策略函数可表示为：</p>

<p><span  class="math">\[
\pi(a \mid s, {\theta})=\mathrm{P}_{\mathrm{r}}\left\{A_{t}=a \mid S_{t}=s, {\theta}_{t}={\theta}\right\}
\]</span></p>

<p>使用上述策略生成马尔可夫过程的一个轨迹<span  class="math">\(\tau=\left({s}_{1}, {a}_{1}, \ldots, {s}_{T}, {a}_{T}\right)\)</span>的概率为：</p>

<p><span  class="math">\[
\underbrace{p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)}_{\pi_{\theta}(\tau)}=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)
\]</span></p>

<p>在策略梯度算法中，目标是找到一组最佳的参数<span  class="math">\(\theta^*\)</span>来表示策略函数使得累计奖励的期望最大，即：</p>

<p><span  class="math">\[
\theta^{\star}=\arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\]</span></p>

<p>当然，相对于平均奖励的目标函数，也存在有起始状态的目标函数。这种情况相当于对各个时间步的奖励进行加权。在这里我们仅仅考虑平均奖励目标函数。</p>

<p>令累积奖励为<span  class="math">\(R(\tau)=\sum_{t=1}^{T} r\left(s_{t}, a_{t}\right)\)</span>，则目标函数为：</p>

<p><span  class="math">\[
J\left(\pi_{\theta}\right)=\underset{\tau \sim \pi_{\theta}}{\mathrm{E}}[R(\tau)]
\]</span></p>

<p>对<span  class="math">\(J\left(\pi_{\theta}\right)\)</span>求梯度可以得到：</p>

<p><span  class="math">\[
\begin{aligned}
\nabla_{\theta} J(\theta) &=\int \nabla_{\theta} \pi_{\theta}(\tau) r(\tau) d \tau \\
&=\int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau) d \tau \\
&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)\right]
\end{aligned}
\]</span></p>

<p>由于有<span  class="math">\(\pi_{\theta}(\tau) = p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)\)</span>，因此将<span  class="math">\(p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)\)</span>对数化后可代入上式。对数化后有：</p>

<p><span  class="math">\[
\log \pi_{\theta}(\tau)=\log p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)+\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)
\]</span></p>

<p>将上述对数化后的式子代入求导，因为<span  class="math">\(\log p\left(\mathbf{s}_{1}\right)\)</span>以及<span  class="math">\(\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)\)</span>都与<span  class="math">\(\theta\)</span>无关，因此求导后可以消去。最终得到：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]
\]</span></p>

<p>使用蒙特卡洛采样可以近似求解。根据策略<span  class="math">\(\pi_{\theta}\)</span>生成<span  class="math">\(N\)</span>条轨迹后，由蒙特卡洛采样有：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
\]</span></p>

<p>求得梯度后可以使用梯度上升法更新策略参数值。</p>

<h3 id="reinforce算法">REINFORCE算法<a href="#reinforce算法" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>根据上述式子可以使用蒙特卡洛法更新策略参数。</p>

<p>输入：<span  class="math">\(N\)</span>个蒙特卡罗完整序列,训练步长<span  class="math">\(\alpha\)</span></p>

<p>输出：策略函数的参数<span  class="math">\(\theta\)</span></p>

<ol>
<li><p>初始化<span  class="math">\(\theta\)</span></p></li>

<li><p>开始循环</p>

<ol>
<li><p>采样一条完整的序列，并计算每个时间位置<span  class="math">\(t\)</span>的状态价值<span  class="math">\(v_{t}\)</span></p></li>

<li><p>对于每个时间位置<span  class="math">\(t\)</span>，使用梯度上升法更新策略函数的参数<span  class="math">\(\theta\)</span>：</p></li>
</ol>

<p><span  class="math">\[
  \theta=\theta+\alpha \nabla_{\theta} \log \pi_{\theta}\left(s_{t}, a_{t}\right) v_{t}
  \]</span></p></li>

<li><p>返回策略函数的参数<span  class="math">\(\theta\)</span></p></li>
</ol>

<p>对于离散空间，最常用的策略函数为softmax函数。即：</p>

<p><span  class="math">\[
\pi_{\theta}(s, a)=\frac{e^{\phi(s, a)^{T_{\theta}}}}{\sum_{b} e^{\phi(s, b)^{T} \theta}}
\]</span></p>

<p>这种情况下，其梯度为：</p>

<p><span  class="math">\[
\nabla_{\theta} \log \pi_{\theta}(s, a)=\phi(s, a)-\mathbb{E}_{\pi \theta}[\phi(s, .)]
\]</span></p>

<p>对于连续空间，可以使用高斯分布函数作为策略函数。这种情况下通常使用神经网络预测分布的均值<span  class="math">\(\mu(s)\)</span>，然后结合给定的方差<span  class="math">\(\sigma^{2}\)</span>构造分布<span  class="math">\(a \sim \mathcal{N}\left(\mu(s), \sigma^{2}\right)\)</span>。</p>

<h3 id="更新式的几何意义">更新式的几何意义<a href="#更新式的几何意义" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>显而易见，<span  class="math">\(\log \pi_{\theta}\left(s_{t}, a_{t}\right)\)</span>是策略<span  class="math">\(\pi\)</span>的对数似然表示。因此，<span  class="math">\( \nabla_{\theta} \log \pi_{\theta}\left(s_{t}, a_{t}\right)\)</span>表示了状态<span  class="math">\(S_{t}\)</span>下执行动作<span  class="math">\(A_{t}\)</span>的对数似然的梯度方向。而<span  class="math">\(v_t\)</span>控制了更新的幅度。相比之下，价值越大的动作更新的幅度越大，即下次遇到同样情况时执行此动作的概率会变高。</p>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2021 <a href="https://sayuri2333.github.io/"></a> Sayuri2333 &#183; <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://sayuri2333.github.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
	</footer>



	<script src="https://sayuri2333.github.io/js/bundle.min.4a9a0ac3d2217822c7865b4161e6c2a71de1d70492264337755427898dd718f6.js" integrity="sha256-SpoKw9IheCLHhltBYebCpx3h1wSSJkM3dVQniY3XGPY=" crossorigin="anonymous"></script>
	

</body>

</html>
