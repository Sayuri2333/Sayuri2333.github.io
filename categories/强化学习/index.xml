<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习 on SayuriBlog</title>
    <link>/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习 on SayuriBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2020 Sayuri2333.</copyright>
    <lastBuildDate>Fri, 05 Mar 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>QR-DQN</title>
      <link>/post/qr-dqn/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/qr-dqn/</guid>
      <description>C51算法的缺陷 C51算法中虽然证明了\(\mathcal{T}\)Bellman算子在使用分布时是满足\(\gamma -contracti</description>
    </item>
    
    <item>
      <title>C51</title>
      <link>/post/c51/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/c51/</guid>
      <description>经典强化学习与值分布强化学习的区别 由于状态转移的随机性，状态表示的混叠效应（编码状态时带来的信息丢失）以及函数逼近的引入（使用函数表示状态价</description>
    </item>
    
    <item>
      <title>DDPG</title>
      <link>/post/ddpg/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/ddpg/</guid>
      <description>DDPG的改进 DDPG可以被视为一个对传统PG算法的改进。在Policy Gradient一文中，我们提到了REINFORCE算法，它实际上与</description>
    </item>
    
    <item>
      <title>Actor-Critic</title>
      <link>/post/actor-critic/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/actor-critic/</guid>
      <description>Policy Gradient的改进 在Policy Gradient的REINFORCE算法实现中，如果某个状态下采样到的复数个动作都取得了正的奖励时，那</description>
    </item>
    
    <item>
      <title>Noisy Network</title>
      <link>/post/noisynet/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/noisynet/</guid>
      <description>背景 对于DRL中面对的exploration-exploition问题，目前广泛使用以下两种方法： ε-greedy法 通常应用于DQN类算法中</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>/post/policy-gradient/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/policy-gradient/</guid>
      <description>DQN类算法的不足 无法表示随机策略。 输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。 无法表示</description>
    </item>
    
    <item>
      <title>Deep Recurrent Q Network</title>
      <link>/post/drqn/</link>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/drqn/</guid>
      <description>背景 在DQN的原文中，需要将游戏最近4帧的图像作为Q网络的输入。这是因为仅仅凭借1帧的画面无法判断物体运动速度和方向等的相关信息。但是在某些</description>
    </item>
    
    <item>
      <title>Wine Quality Prediction With Random Forest</title>
      <link>/post/solve_project/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/solve_project/</guid>
      <description>Introduction For winemakers, it is very important to know how to judge the quality of wine by its chemical components. In this report, we analyze the white wine dataset, use random forest algorithm and logistic regression algorithm to build models to distinguish the quality of wine, and determine the importance of each chemical component for wine quality judgment by its weights in both algorithm. Exploratory Data Analysis First import the</description>
    </item>
    
    <item>
      <title>Dueling DQN算法</title>
      <link>/post/duelingdqn/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/duelingdqn/</guid>
      <description>Dueling DQN的改进 Dueling DQN算法主要针对传统DQN算法对于状态价值的评估做出了改进。在传统的DQN算法中，Q网络能够预测给定\((s,a)\)的状</description>
    </item>
    
    <item>
      <title>Prioritized Experience Replay</title>
      <link>/post/per/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/per/</guid>
      <description>PER的由来 在PER之前，像DQN（Nature 2015）以及Double DQN等Deep Q-learning方法都是通过经验回放的手段进行</description>
    </item>
    
    <item>
      <title>Double DQN算法</title>
      <link>/post/doubledqn/</link>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/doubledqn/</guid>
      <description>DQN 以及其改进 在原版Q-learning算法中，Q网络的优化目标为： \[ Y_{t}^{\mathrm{Q}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) \] 基于此优化目标建立均方误差损失函数，优化迭代式如下：</description>
    </item>
    
    <item>
      <title>重要性采样的问题</title>
      <link>/post/ddpg%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/ddpg%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98/</guid>
      <description>重要性采样在策略梯度类算法的应用 PPO算法是重要性采样在策略梯度类算法中应用的典型成果。策略梯度类算法通常以最大化期望奖励（Expected</description>
    </item>
    
    <item>
      <title>Deep Q-learning算法</title>
      <link>/post/deep-q-learning/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/deep-q-learning/</guid>
      <description>价值函数的近似表示 之前介绍的强化学习方法使用的都是有限个状态集合\(S\)，而当遇到连续的状态时，则需要价值函数的近似表示。 价值函数的近似表</description>
    </item>
    
    <item>
      <title>Q-learning算法</title>
      <link>/post/q-learning/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/q-learning/</guid>
      <description>Q-learning算法的引入 Q-learning算法的步骤与SARSA算法大致相同，唯一不同的地方在于SARSA算法在更新价值函数时会使用</description>
    </item>
    
    <item>
      <title>SARSA算法</title>
      <link>/post/sarsa/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/sarsa/</guid>
      <description>SARSA算法的引入 SARSA算法不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题的求解与蒙特卡洛法相似，即通</description>
    </item>
    
    <item>
      <title>时序差分法求解</title>
      <link>/post/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/</guid>
      <description>时序差分法简介 时序差分法与蒙特卡洛法相似，都是不基于模型的强化学习问题求解方法。时序差分法使用不完整的状态序列近似求出给定状态的收获。回顾贝</description>
    </item>
    
    <item>
      <title>蒙特卡洛法求解</title>
      <link>/post/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95/</guid>
      <description>不基于模型的强化学习问题定义 在动态规划一文中，我们提到强化学习的两个基本问题：预测问题与控制问题。在处理上述两种问题的时候，其状态转化概率矩</description>
    </item>
    
    <item>
      <title>动态规划求解强化学习问题</title>
      <link>/post/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</guid>
      <description>动态规划与强化学习的关系 动态规划的关键点在于：（1）问题的最优解可以由若干个小问题的最优解构成，即通过寻找子问题的最优解可以得到问题的最优解</description>
    </item>
    
    <item>
      <title>强化学习基础</title>
      <link>/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid>
      <description>强化学习基本要素 环境状态\(S\), t时刻环境的状态\(S_t\)是它的环境状态集中的某一个状态。 个体动作\(A\)， t时刻个体采取的动作\</description>
    </item>
    
    <item>
      <title>马尔可夫决策过程</title>
      <link>/post/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</guid>
      <description>引入马尔可夫决策过程(MDP) 在强化学习基础介绍的强化学习要素中，状态转移概率\(P_{ss&#39;}^a\)不仅与上一个状态有关，也与之前所有状</description>
    </item>
    
    <item>
      <title>强化学习入门资料</title>
      <link>/post/%E4%B9%A6%E7%9B%AE/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/%E4%B9%A6%E7%9B%AE/</guid>
      <description>阅读书目 PRML / 国内有非正式版翻译 Reinforcement learning: An Introduction / 有中文版 Artificial intelligence: A Modern Approach Part I-III / 有中文 翻译一般 比较general的书 Deep learning / 花书 Artificial Intelligence and Games / 有中文版 翻译质量还行</description>
    </item>
    
  </channel>
</rss>