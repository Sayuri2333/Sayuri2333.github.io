<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Actor-Critic&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">Actor-Critic</h1><p class="article date">Wednesday, March 3, 2021</p></section><article class="article markdown-body"><h3 id="policy-gradient的改进">Policy Gradient的改进</h3>

<p>在Policy Gradient的REINFORCE算法实现中，如果某个状态下采样到的复数个动作都取得了正的奖励时，那么相对获得的奖励较多的动作就会得到更高的采样概率提升。但在实际状态下，通常无法采样到一个状态下的所有动作。因此，在这种情况下算法最终会逐渐偏向于一开始取得正向奖励的动作，其他潜在的更优动作则无法被发现。</p>

<p>为了解决上述的问题，最好的方法就是让奖励<span  class="math">\(R(\tau)\)</span>有正有负。在REINFORCE算法中，目标函数的梯度为：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
\]</span></p>

<p>可以简化写为：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{n=1}^{N} \nabla_{\theta} \log P(\tau, \theta) R(\tau)
\]</span></p>

<p>让奖励有正有负，最好的方法就是在奖励上减去一个固定的值。即把上述式子改为：</p>

<p><span  class="math">\[
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{n=1}^{N} \nabla_{\theta} \log p_{\theta}(\tau)\left[R\left(\tau^{n}\right)-b\right]
\]</span></p>

<p>其中<span  class="math">\(b\)</span>为一个固定的常数。可以证明减去一个<span  class="math">\(b\)</span>常数不影响整个梯度的计算。</p>

<p><span  class="math">\[
E\left[\nabla_{\theta} \log p_{\theta}(\tau) b\right]=\int p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) b d \tau=\int \nabla_{\theta} p_{\theta}(\tau) b d \tau=b \nabla_{\theta} \int p_{\theta}(\tau) d \tau=b \nabla_{\theta} 1=0
\]</span></p>

<p>通常来说，常数<span  class="math">\(b\)</span>会使用轨迹整体奖励的均值：</p>

<p><span  class="math">\[
b=\frac{1}{N} \sum_{n=1}^{N} R(\tau)
\]</span></p>

<h3 id="actorcritic">Actor-Critic</h3>

<p>由<span  class="math">\(E\left[\nabla_{\theta} \log p_{\theta}(\tau) b\right]\)</span>的计算可知，只要常数<span  class="math">\(b\)</span>的取值与策略<span  class="math">\(\pi\)</span>无关就能够满足上述等式。当使用状态值函数<span  class="math">\(\hat{V}\left(s_{t}\right)\)</span>作为常数时，策略梯度可以写成：</p>

<p><span  class="math">\[
\nabla J(\theta)=\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{i, t} \mid s_{i, t}\right)\left(\hat{Q}\left(s_{t}, a_{t}\right)-\hat{V}\left(s_{t}\right)\right)
\]</span></p>

<p>其中，<span  class="math">\(\hat{Q}\left(s_{t}, a_{t}\right)-\hat{V}\left(s_{t}\right)\)</span>也可以被称为优势函数<span  class="math">\(\hat{A}\left(s_{t}, a_{t}\right)\)</span>。优势函数可以用来度量在某个状态下选取某个动作相对于平均性能的差值。通过贝尔曼方程，我们可以得到：</p>

<p><span  class="math">\[
Q\left(s_{t}, a_{t}\right)=r\left(s_{t}, a_{t}\right)+\sum_{s_{t+1}} P\left(s_{t+1} \mid s_{t}, a_{t}\right)\left[V\left(s_{t+1}\right)\right]
\]</span></p>

<p>我们可以通过采样的方式估计状态转移概率，因此有：</p>

<p><span  class="math">\[
Q\left(s_{t}, a_{t}\right) \approx r\left(s_{t}, a_{t}\right)+V\left(s_{t+1}\right)
\]</span></p>

<p>因此策略梯度公式可以进一步写成：</p>

<p><span  class="math">\[
\nabla J(\theta)=\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{i, t} \mid s_{i, t}\right)\left(r\left(s_{t}, a_{t}\right)+\hat{V}\left(s_{t+1}\right)-\hat{V}\left(s_{t}\right)\right)
\]</span></p>

<p>其中，<span  class="math">\(N\)</span>为采样轨迹数。在上式中，我们需要估计状态价值函数<span  class="math">\(V(s_{t})\)</span>的值。因此，我们可以使用一个Critic网络来估计状态价值，并使用Actor网络给出具体策略。</p>

<h3 id="actorcritic的流程">Actor-Critic的流程</h3>

<p>输入：迭代轮数<span  class="math">\(T\)</span>，状态特征维度<span  class="math">\(n\)</span>，动作集<span  class="math">\(A\)</span>，步长<span  class="math">\(\alpha, \beta\)</span>，衰减因子<span  class="math">\(\gamma\)</span>，探索率<span  class="math">\(\varepsilon\)</span>，Actor网络以及Critic网络</p>

<p>输出：Actor网络参数<span  class="math">\(\theta\)</span>，Critic网络参数<span  class="math">\(\omega\)</span></p>

<ol>
<li><p>随机初始化所有的状态和动作对应的价值<span  class="math">\(Q\)</span></p></li>

<li><p>进行<span  class="math">\(T\)</span>轮迭代</p>

<ol>
<li><p>初始化<span  class="math">\(S\)</span>为当前状态序列的第一个状态，将其输入Actor网络并得到动作，获得<span  class="math">\(\{S, A, S', R\}\)</span></p></li>

<li><p>将<span  class="math">\(S,S'\)</span>输入Critic网络，得到状态价值<span  class="math">\(V(S), V(S')\)</span></p></li>

<li><p>计算TD误差<span  class="math">\(\delta = R+\gamma V\left(S^{\prime}\right)-V(S)\)</span></p></li>

<li><p>使用TD误差并结合步长<span  class="math">\(\beta\)</span>，衰减因子<span  class="math">\(\gamma\)</span>更新Critic网络参数<span  class="math">\(\omega\)</span></p></li>

<li><p>更新Actor网络参数<span  class="math">\(\theta\)</span>：</p></li>
</ol>

<p><span  class="math">\[
  \theta = \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(S_t, A) \delta
  \]</span></p></li>
</ol>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/ddpg/"><span class="li">&larr;</span>DDPG</a></p><p><a class="link" href="/post/noisynet/"><span class="li">&rarr;</span>Noisy Network</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>