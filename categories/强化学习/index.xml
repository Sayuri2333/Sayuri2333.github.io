<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习 on SayuriBlog</title>
    <link>https://example.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习 on SayuriBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2020 Sayuri2333.</copyright>
    <lastBuildDate>Thu, 29 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://example.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>重要性采样的问题</title>
      <link>https://example.com/post/ddpg%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/ddpg%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98/</guid>
      <description>重要性采样在策略梯度类算法的应用 PPO算法是重要性采样在策略梯度类算法中应用的典型成果。策略梯度类算法通常以最大化期望奖励（Expected</description>
    </item>
    
    <item>
      <title>Deep Q-learning算法</title>
      <link>https://example.com/post/deep-q-learning/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/deep-q-learning/</guid>
      <description>价值函数的近似表示 之前介绍的强化学习方法使用的都是有限个状态集合\(S\)，而当遇到连续的状态时，则需要价值函数的近似表示。 价值函数的近似表</description>
    </item>
    
    <item>
      <title>Q-learning算法</title>
      <link>https://example.com/post/q-learning/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/q-learning/</guid>
      <description>Q-learning算法的引入 Q-learning算法的步骤与SARSA算法大致相同，唯一不同的地方在于SARSA算法在更新价值函数时会使用</description>
    </item>
    
    <item>
      <title>SARSA算法</title>
      <link>https://example.com/post/sarsa/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/sarsa/</guid>
      <description>SARSA算法的引入 SARSA算法不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题的求解与蒙特卡洛法相似，即通</description>
    </item>
    
    <item>
      <title>时序差分法求解</title>
      <link>https://example.com/post/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/</guid>
      <description>时序差分法简介 时序差分法与蒙特卡洛法相似，都是不基于模型的强化学习问题求解方法。时序差分法使用不完整的状态序列近似求出给定状态的收获。回顾贝</description>
    </item>
    
    <item>
      <title>蒙特卡洛法求解</title>
      <link>https://example.com/post/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95/</guid>
      <description>不基于模型的强化学习问题定义 在动态规划一文中，我们提到强化学习的两个基本问题：预测问题与控制问题。在处理上述两种问题的时候，其状态转化概率矩</description>
    </item>
    
    <item>
      <title>动态规划求解强化学习问题</title>
      <link>https://example.com/post/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</guid>
      <description>动态规划与强化学习的关系 动态规划的关键点在于：（1）问题的最优解可以由若干个小问题的最优解构成，即通过寻找子问题的最优解可以得到问题的最优解</description>
    </item>
    
    <item>
      <title>强化学习基础</title>
      <link>https://example.com/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid>
      <description>强化学习基本要素 环境状态\(S\), t时刻环境的状态\(S_t\)是它的环境状态集中的某一个状态。 个体动作\(A\)， t时刻个体采取的动作\</description>
    </item>
    
    <item>
      <title>马尔可夫决策过程</title>
      <link>https://example.com/post/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</guid>
      <description>引入马尔可夫决策过程(MDP) 在强化学习基础介绍的强化学习要素中，状态转移概率\(P_{ss&#39;}^a\)不仅与上一个状态有关，也与之前所有状</description>
    </item>
    
    <item>
      <title>强化学习入门资料</title>
      <link>https://example.com/post/%E4%B9%A6%E7%9B%AE/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/%E4%B9%A6%E7%9B%AE/</guid>
      <description>阅读书目 PRML / 国内有非正式版翻译 Reinforcement learning: An Introduction / 有中文版 Artificial intelligence: A Modern Approach Part I-III / 有中文 翻译一般 比较general的书 Deep learning / 花书 Artificial Intelligence and Games / 有中文版 翻译质量还行</description>
    </item>
    
  </channel>
</rss>