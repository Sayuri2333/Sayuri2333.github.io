<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>SARSA算法&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">SARSA算法</h1><p class="article date">Friday, May 8, 2020</p></section><article class="article markdown-body"><h4 id="sarsa算法的引入">SARSA算法的引入</h4>

<p>SARSA算法不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题的求解与蒙特卡洛法相似，即通过价值函数的更新来更新当前的策略，再通过新的策略来产生新的状态和即时奖励从而更新价值函数。</p>

<p>SARSA算法属于在线控制的时序差分法，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是<span  class="math">\(\epsilon\)</span>-贪婪法。</p>

<h4 id="sarsa算法概述">SARSA算法概述</h4>

<p>在迭代的时候，我们首先基于ϵ−贪婪法在当前状态S选择一个动作A，这样系统会转到一个新的状态S′, 同时给我们一个即时奖励R, 在新的状态S′，我们会基于ϵ−贪婪法在状态S′选择一个动作A′，但是注意这时候我们并不执行这个动作A′，只是用来更新的我们的价值函数，价值函数的更新公式是：</p>

<p><span  class="math">\[
Q(S,A) = Q(S,A) + \alpha(R+\gamma Q(S',A') - Q(S,A))
\]</span></p>

<p>其中<span  class="math">\(\gamma\)</span>是衰减因子，<span  class="math">\(\alpha\)</span>是步长。这里和蒙特卡洛算法的主要区别在与收获<span  class="math">\(G_t\)</span>的表达式不同，对于时序差分，收获<span  class="math">\(G_t\)</span>的表达式是<span  class="math">\(R+\gamma Q(S',A')\)</span>。</p>

<h4 id="sarsa算法流程">SARSA算法流程</h4>

<p>算法输入：迭代轮数<span  class="math">\(T\)</span>，状态集<span  class="math">\(S\)</span>，动作集<span  class="math">\(A\)</span>，步长<span  class="math">\(\alpha\)</span>，衰减因子<span  class="math">\(\gamma\)</span>，探索率<span  class="math">\(\epsilon\)</span></p>

<p>输出：所有的状态和动作对应的价值<span  class="math">\(Q\)</span></p>

<ol>
<li>随机初始化所有的状态和动作对应的价值<span  class="math">\(Q\)</span>。对于终止状态其<span  class="math">\(Q\)</span>值初始化为0</li>
<li>for i from i to T, 进行迭代

<ol>
<li>初始化<span  class="math">\(S\)</span>为当前序列的第一个状态，设置<span  class="math">\(A\)</span>为<span  class="math">\(\epsilon\)</span>-贪婪法在当前状态选择的动作</li>
<li>在状态<span  class="math">\(S\)</span>执行当前动作<span  class="math">\(A\)</span>，得到新状态<span  class="math">\(S'\)</span>以及奖励<span  class="math">\(R\)</span></li>
<li>用<span  class="math">\(\epsilon\)</span>-贪婪法在状态<span  class="math">\(S'\)</span>选择新的动作<span  class="math">\(A'\)</span></li>
<li>更新价值函数<span  class="math">\(Q(S,A)\)</span>：<span  class="math">\(Q(S,A) = Q(S,A) + \alpha(R+\gamma Q(S',A') - Q(S,A))\)</span></li>
<li><span  class="math">\(S=S', A=A'\)</span></li>
<li>如果<span  class="math">\(S\)</span>是终止状态，则当前轮迭代完毕，否则转到步骤2.2</li>
</ol></li>
</ol>

<p>一般来说，<span  class="math">\(\alpha\)</span>一般需要随着迭代的进行而逐渐变小，这样才能保证动作价值函数的收敛。</p>

<h4 id="sarsa算法实践">SARSA算法实践</h4>

<p>使用SARSA算法解决Windy Gridworld问题。具体实践参照我的<a href="https://github.com/Sayuri2333/reinforcement_learning">github</a>。</p>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/q-learning/"><span class="li">&larr;</span>Q-learning算法</a></p><p><a class="link" href="/post/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/"><span class="li">&rarr;</span>时序差分法求解</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>