<!DOCTYPE html>
<html lang="zh-cn">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#494f5c">
	<meta name="msapplication-TileColor" content="#494f5c">
<meta itemprop="name" content="Double DQN算法">
<meta itemprop="description" content="DQN 以及其改进 在原版Q-learning算法中，Q网络的优化目标为： \[ Y_{t}^{\mathrm{Q}} \equiv R_{t&#43;1}&#43;\gamma \max _{a} Q\left(S_{t&#43;1}, a ; \boldsymbol{\theta}_{t}\right) \] 基于此优化目标建立均方误差损失函数，优化迭代式如下：">
<meta itemprop="datePublished" content="2020-10-29T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-10-29T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="992">



<meta itemprop="keywords" content="" /><meta property="og:title" content="Double DQN算法" />
<meta property="og:description" content="DQN 以及其改进 在原版Q-learning算法中，Q网络的优化目标为： \[ Y_{t}^{\mathrm{Q}} \equiv R_{t&#43;1}&#43;\gamma \max _{a} Q\left(S_{t&#43;1}, a ; \boldsymbol{\theta}_{t}\right) \] 基于此优化目标建立均方误差损失函数，优化迭代式如下：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sayuri2333.github.io/post/doubledqn/" />
<meta property="article:published_time" content="2020-10-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Double DQN算法"/>
<meta name="twitter:description" content="DQN 以及其改进 在原版Q-learning算法中，Q网络的优化目标为： \[ Y_{t}^{\mathrm{Q}} \equiv R_{t&#43;1}&#43;\gamma \max _{a} Q\left(S_{t&#43;1}, a ; \boldsymbol{\theta}_{t}\right) \] 基于此优化目标建立均方误差损失函数，优化迭代式如下："/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>Double DQN算法</title>
	<link rel="stylesheet" href="https://sayuri2333.github.io/css/style.min.eac77496566fd7d5768fd650ddb0b2b181ca6a2d7c5fdd6fe6b8ba4bf47e566f.css" integrity="sha256-6sd0llZv19V2j9ZQ3bCysYHKai18X91v5ri6S/R+Vm8=" crossorigin="anonymous">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://sayuri2333.github.io">SayuriBlog</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://sayuri2333.github.io/post/">Posts</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-social hide-in-mobile"><a href="https://github.com/Sayuri2333" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://sayuri2333.github.io/post/">Posts</a></li>
		</ul>
	</div>


	<main class="site-main section-inner thin animated fadeIn faster">
		<h1>Double DQN算法</h1>
		<div class="content">
			<h3 id="dqn-以及其改进">DQN 以及其改进<a href="#dqn-以及其改进" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>在原版Q-learning算法中，Q网络的优化目标为：</p>

<p><span  class="math">\[
Y_{t}^{\mathrm{Q}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right)
\]</span></p>

<p>基于此优化目标建立均方误差损失函数，优化迭代式如下：</p>

<p><span  class="math">\[
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha\left(Y_{t}^{\mathrm{Q}}-Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right)\right) \nabla_{\boldsymbol{\theta}_{t}} Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right)
\]</span></p>

<p>与原版（2013年）发布的Deep Q-learning算法相比，与2015年发布的Deep Q-learning算法对原版算法进行了改进。2015年的改进版本DQN有两个较为重要的特点：目标网络的使用以及经验回放池的引入。目标网络与原版网络具有相同的结构，令原版网络的参数为<span  class="math">\(\theta_{t}\)</span>，目标网络的参数为<span  class="math">\(\theta_{t}^{-}\)</span>，则改进后的DQN的优化目标为：</p>

<p><span  class="math">\[
Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}^{-}\right)
\]</span></p>

<p>其中目标网络每<span  class="math">\(\tau\)</span>步从原网络复制参数，并在剩下的时间步中保持不变。</p>

<h3 id="dqn算法的缺陷">DQN算法的缺陷<a href="#dqn算法的缺陷" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>上述两种Q网络的优化目标都存在一定缺陷，即使用同一个网络的value来选择和评价action。在<span  class="math">\(Y_{t}^{\mathrm{Q}}\)</span>中，使用原始Q网络根据max策略选择action，并使用同一个网络的值对此action进行评价；在<span  class="math">\(Y_{t}^{\mathrm{DQN}}\)</span>中进行的是相同的操作，只是其发生在目标Q网络中。这种情况下通常会选择过估计的value值，从而对整个网络的Q值进行过分乐观估计。为了解决上述问题，Double DQN对action选择以及评估进行了解耦，使用当前网络选择最优action，并使用目标网络计算对应value。改进后的优化目标为：</p>

<p><span  class="math">\[
Y_{t}^{\text {DoubleQ }} \equiv R_{t+1}+\gamma Q\left(S_{t+1}, \operatorname{argmax} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) ; \boldsymbol{\theta}_{t}^{\prime}\right)
\]</span></p>

<h3 id="过优估计">过优估计<a href="#过优估计" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>以下为结合知乎回答<a href="https://www.zhihu.com/question/394866647/answer/1265905876">1</a>以及<a href="https://www.zhihu.com/question/67758340/answer/256406504">2</a>的个人理解。</p>

<p>首先我们有公式：</p>

<p><span  class="math">\[
E\left(\max \left(y_{1}, y_{2}, \ldots\right)\right) \geq \max \left(E\left(y_{1}, y_{2}, \ldots\right)\right)
\]</span></p>

<p>直观描述为对一系列数求最大值再求平均通常比先求平均值再求最大值要大。</p>

<p>由于Q-learning算法通常以拟合Bellman最优函数为目标：</p>

<p><span  class="math">\[
Q_{\pi}(s, a)=R(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim p(s, a)}\left[\max _{a^{\prime}} Q_{\pi}\left(s^{\prime}, a^{\prime}\right)\right]
\]</span></p>

<p>但理想情况下<span  class="math">\(Q_{\pi}(s, a)\)</span>的值应该等同于下式：</p>

<p><span  class="math">\[
Q_{\pi}(s, a) = \mathbb{E}(R(s, a)) + \gamma \mathbb{E}_{s^{\prime} \sim p(s, a)}\left[\max_{a^{\prime} \sim \pi} \mathbb{E}[Q_{\pi}\left(s^{\prime}, a^{\prime}\right)]\right]
\]</span></p>

<p>即理想情况下应该先求出对于可能出现的状态<span  class="math">\(s^{\prime}\)</span>以及可能的动作<span  class="math">\(a^{\prime}\)</span>的Q值的期望，再对期望求最大值并根据<span  class="math">\(s^{\prime}\)</span>出现的概率求总期望。根据上面的不等式有：</p>

<p><span  class="math">\[
\mathbb{E}[\max _{a^{\prime}} Q_{\pi}\left(s^{\prime}, a^{\prime}\right)] \geq \max_{a^{\prime} \sim \pi} \mathbb{E}[Q_{\pi}\left(s^{\prime}, a^{\prime}\right)]
\]</span></p>

<p>因此Q-learning对于最优函数的拟合总会出现乐观估计。</p>

<h3 id="算法流程">算法流程<a href="#算法流程" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>

<p>参考<a href="https://www.cnblogs.com/pinard/p/9778063.html">Pinard的博客</a>。</p>

<p>具体Python实现参考<a href="https://github.com/Sayuri2333/reinforcement_learning">我的github</a>。</p>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2021 <a href="https://sayuri2333.github.io"></a> Sayuri2333 &#183; <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://sayuri2333.github.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
	</footer>



	<script src="https://sayuri2333.github.io/js/bundle.min.4a9a0ac3d2217822c7865b4161e6c2a71de1d70492264337755427898dd718f6.js" integrity="sha256-SpoKw9IheCLHhltBYebCpx3h1wSSJkM3dVQniY3XGPY=" crossorigin="anonymous"></script>
	

</body>

</html>
