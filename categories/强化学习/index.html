<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>强化学习&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><link rel="alternate" type="application/rss+xml" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" title="SayuriBlog" /><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header"><h1>强化学习</h1></section><ul class="note list"><li class="item"><a class="note" href="/post/qr-dqn/">
            <p class="note title">QR-DQN</p><p class="note date">Friday, March 5, 2021</p><p class="note content">C51算法的缺陷 C51算法中虽然证明了\(\mathcal{T}\)Bellman算子在使用分布时是满足\(\gamma -contracti<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/c51/">
            <p class="note title">C51</p><p class="note date">Thursday, March 4, 2021</p><p class="note content">经典强化学习与值分布强化学习的区别 由于状态转移的随机性，状态表示的混叠效应（编码状态时带来的信息丢失）以及函数逼近的引入（使用函数表示状态价<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/ddpg/">
            <p class="note title">DDPG</p><p class="note date">Thursday, March 4, 2021</p><p class="note content">DDPG的改进 DDPG可以被视为一个对传统PG算法的改进。在Policy Gradient一文中，我们提到了REINFORCE算法，它实际上与<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/actor-critic/">
            <p class="note title">Actor-Critic</p><p class="note date">Wednesday, March 3, 2021</p><p class="note content">Policy Gradient的改进 在Policy Gradient的REINFORCE算法实现中，如果某个状态下采样到的复数个动作都取得了正的奖励时，那<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/noisynet/">
            <p class="note title">Noisy Network</p><p class="note date">Tuesday, March 2, 2021</p><p class="note content">背景 对于DRL中面对的exploration-exploition问题，目前广泛使用以下两种方法： ε-greedy法 通常应用于DQN类算法中<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/policy-gradient/">
            <p class="note title">Policy Gradient</p><p class="note date">Tuesday, March 2, 2021</p><p class="note content">DQN类算法的不足 无法表示随机策略。 输出值（Q值）的微小改变可能会导致某一个动作被选中或者不选中，这种不连续的变化会影响算法的收敛。 无法表示<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/drqn/">
            <p class="note title">Deep Recurrent Q Network</p><p class="note date">Saturday, January 30, 2021</p><p class="note content">背景 在DQN的原文中，需要将游戏最近4帧的图像作为Q网络的输入。这是因为仅仅凭借1帧的画面无法判断物体运动速度和方向等的相关信息。但是在某些<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/solve_project/">
            <p class="note title">Wine Quality Prediction With Random Forest</p><p class="note date">Thursday, January 21, 2021</p><p class="note content">Introduction For winemakers, it is very important to know how to judge the quality of wine by its chemical components. In this report, we analyze the white wine dataset, use random forest algorithm and logistic regression algorithm to build models to distinguish the quality of wine, and determine the importance of each chemical component for wine quality judgment by its weights in both algorithm. Exploratory Data Analysis First import the<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/duelingdqn/">
            <p class="note title">Dueling DQN算法</p><p class="note date">Friday, October 30, 2020</p><p class="note content">Dueling DQN的改进 Dueling DQN算法主要针对传统DQN算法对于状态价值的评估做出了改进。在传统的DQN算法中，Q网络能够预测给定\((s,a)\)的状<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li><li class="item"><a class="note" href="/post/per/">
            <p class="note title">Prioritized Experience Replay</p><p class="note date">Friday, October 30, 2020</p><p class="note content">PER的由来 在PER之前，像DQN（Nature 2015）以及Double DQN等Deep Q-learning方法都是通过经验回放的手段进行<span class="mldr">&mldr;more</span></p></a><p class="note labels"><a class="category" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></p></li></ul><div class="pagination">
    <ul><li><a class="active" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">1</a></li><li><a class="" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/page/2/">2</a></li><li><a class="" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/page/3/">3</a></li></ul>
</div></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>