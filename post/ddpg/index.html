<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.89.1" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>DDPG&nbsp;&ndash;&nbsp;SayuriBlog</title><link rel="stylesheet" href="/css/core.min.648a6d12e2a3dde16e849ff7df9be3ae5560eafe1a16f581e9555eea238e863fe56d7308f54ae31f49106dd309b237eb.css" integrity="sha384-ZIptEuKj3eFuhJ/335vjrlVg6v4aFvWB6VVe6iOOhj/lbXMI9UrjH0kQbdMJsjfr"><body>
    <div class="base-body"><code class="html"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"/></code>
<section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/img/logo.jpg" alt /><span class="site name">SayuriBlog</span></a></span>
        <span class="header right-side"></span></div></section><div id="content"><section class="article header">
    <h1 class="article title">DDPG</h1><p class="article date">Thursday, March 4, 2021</p></section><article class="article markdown-body"><h3 id="ddpg的改进">DDPG的改进</h3>

<p>DDPG可以被视为一个对传统PG算法的改进。在Policy Gradient一文中，我们提到了REINFORCE算法，它实际上与传统的蒙特卡洛法有些相似，都需要采样一个完整序列后才能进行训练。不仅如此，其每次训练需要重新采样，也就是说是一种on-policy的算法。DDPG算法对PG算法进行了改进，使其能够实现off-policy训练，并且能够使用经验回放池内的经验进行分组训练。相比于传统PG算法，DDPG提出了一种确定性的策略，也就是说DDPG认为针对某种状态，需要执行的动作是一定的。具体与PG算法的区别在重要性采样那一篇文章中有进行阐述。除此之外，DDPG还实现了soft update，相对来说在性能上也给出了一定的提升。</p>

<h3 id="ddpg流程">DDPG流程</h3>

<p>主要流程与基于PG的AC算法相同，不同之处在于actor网络的更新方式以及对于DQN中Double Q网络思想的应用。在DDPG中，actor网络的策略梯度为：</p>

<p><span  class="math">\[
\begin{aligned}
\nabla_{\theta} J\left(\mu_{\theta}\right) &=\left.\int_{\mathcal{S}} \rho^{\mu}(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu_{\theta}(s)} \mathrm{d} s \\
&=\mathbb{E}_{s \sim \rho^{\mu}}\left[\left.\nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu_{\theta}(s)}\right]
\end{aligned}
\]</span></p>

<p>也就是说，actor网络使用的目标函数<span  class="math">\(J\left(\mu_{\theta}\right)\)</span>为：</p>

<p><span  class="math">\[
J\left(\mu_{\theta}\right) = \mathbb{E}_{s \sim \rho^{\mu}}\left[ Q(s, \mu_{\theta}(s))\right]
\]</span></p>

<p>在实际应用中，我们使用的是损失函数，因此需要对上述目标函数取负号。</p>

<p>DDPG具体流程如下：</p>

<p><figure><img src="/image/DDPG.png" alt=""></figure></p>
</article><section class="article labels"><a class="category" href=/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></section><section class="article navigation"><p><a class="link" href="/post/c51/"><span class="li">&larr;</span>C51</a></p><p><a class="link" href="/post/actor-critic/"><span class="li">&rarr;</span>Actor-Critic</a class="link">
    </p></section></div><code class="html"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe"crossorigin="anonymous"onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</code>
<section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 <a href="https://github.com/Sayuri2333" target="_blank">Sayuri2333</a>.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>